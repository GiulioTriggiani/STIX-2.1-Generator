{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation for GenAI-Powered STIX 2.1 Generator**\n",
        "\n",
        "**Notebook Version:** 9.0  \n",
        "**Author:** Giulio Triggiani  \n",
        "**Python Version:** >= 3.8  \n",
        "**Key Libraries:** `stix2`, `stix2validator`\n",
        "\n",
        "---\n",
        "\n",
        "## **Objective**\n",
        "The objective is to **quantitatively evaluate** the performance of GenAI_STIX_2_1_Generator, an LLM-based tool for the automatic generation of Cyber Threat Intelligence reports in STIX 2.1 format.  \n",
        "\n",
        "To measure the effectiveness of the generator, this script performs a comparison between a bundle automatically generated by the tool and a reference bundle (‚Äúground truth‚Äù) manually created by a CTI analyst.  \n",
        "\n",
        "The analysis leverages the advanced features of the official `stix2` library, in particular a **semantic comparison** is made between the various STIX objects (SDO, SCO, and SRO) to identify not only literal matches, but also matches in meaning.  \n",
        "\n",
        "The data obtained from this comparison is then used to calculate **standard performance metrics** such as Precision, Recall, and F1-Score, providing a clear and objective assessment of the quality of the generated bundle.\n",
        "\n",
        "## **Workflow Overview**\n",
        "\n",
        "1.   **Setup**: installs the Python libraries needed for validating and manipulating STIX objects;\n",
        "2.   **Libraries and Environment**: imports the required modules and mount Google Drive to access the file and set the threshold for comparison;\n",
        "3.   **Support Functions**: the main functions that perform validation and object extraction;\n",
        "4.   **Metrics**: the function that calculates metrics and various averages\n",
        "5.   **Comparison**: performs the entire process: it browses folders, loads bundles, compares them, and prints the results.\n",
        "6.   **Part 6**: Generation of analysis charts\n",
        "\n"
      ],
      "metadata": {
        "id": "wSrXE79msx4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1**: Setup\n",
        "\n",
        "This block installs the Python libraries needed for validating and manipulating STIX objects."
      ],
      "metadata": {
        "id": "d7ZWDNcrEyDj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn1X6pO5DAuw"
      },
      "outputs": [],
      "source": [
        "# Installation of necessary libraries\n",
        "print(\"--- Installing dependencies ---\")\n",
        "!pip install stix2[semantic] --quiet\n",
        "!pip install stix2-validator --quiet\n",
        "!pip install rapidfuzz --quiet\n",
        "print(\"Installation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2**: Importing Libraries and Setting Up the Environment\n",
        "\n",
        "This block imports the required modules and mounts Google Drive to access the files, you can also set the threshold for comparison.\n",
        "\n",
        "The comparison threshold can have a value between 0 and 100. The similarity threshold is the minimum score that two objects must achieve to be considered equivalent. It establishes ‚Äúhow similar they must be‚Äù to be counted as a match (a True Positive).\n",
        "A value between 80 and 95 is recommended.\n",
        "\n",
        "NOTE: The first time you will be asked for access to Google Drive."
      ],
      "metadata": {
        "id": "_mjmdirsG1cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import and setup of the environment\n",
        "import os\n",
        "import json\n",
        "import subprocess\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "import re # Assicurati che 'import re' sia disponibile (preferibilmente all'inizio della cella)\n",
        "\n",
        "# Import from the stix2 library\n",
        "from stix2.equivalence.object import object_similarity, object_equivalence\n",
        "from stix2 import parse, MemoryStore\n",
        "\n",
        "# To connect to Google Drive in Colab\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"--- Google Drive Mount ---\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Google Drive mounted correctly.\")\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "# Set the base directory on Google Drive where the folders are located\n",
        "BASE_DRIVE_DIR = '/content/drive/MyDrive/Reports_Evaluation'\n",
        "\n",
        "# Set the similarity threshold for considering two objects equivalent (from 0 to 100)\n",
        "SIMILARITY_THRESHOLD = 80\n",
        "\n",
        "# Set the similarity threshold for relationships (da 0 a 100)\n",
        "RELATIONSHIP_SIMILARITY_THRESHOLD = 80"
      ],
      "metadata": {
        "id": "d-tt_czFHOjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3**: Support Functions\n",
        "This cell contains the main functions that perform validation and object extraction.\n",
        "\n",
        "Bundles are validated according to the STIX 2.1 standard."
      ],
      "metadata": {
        "id": "UYWHHh5Ef9-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from collections import defaultdict\n",
        "from copy import deepcopy\n",
        "\n",
        "# Import from validator\n",
        "from stix2validator import validate_file, print_results\n",
        "\n",
        "# Import from the stix2 library\n",
        "from stix2.equivalence.object import object_similarity, object_equivalence\n",
        "\n",
        "def validate_bundle(file_path):\n",
        "    \"\"\"\n",
        "    Validate an STIX 2.1 bundle using the stix2validator library.\n",
        "    Returns True if the bundle is valid, otherwise False.\n",
        "    \"\"\"\n",
        "    print(f\"\\nValidating the bundle: {os.path.basename(file_path)}...\")\n",
        "\n",
        "    # Performs validation and obtains an object with the results\n",
        "    results = validate_file(file_path)\n",
        "\n",
        "    if results.is_valid:\n",
        "        print(f\"‚úÖ Validation of {os.path.basename(file_path)} success.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"‚ùå Validation ERROR for {os.path.basename(file_path)}:\")\n",
        "        # Use the print_results function to display errors in a formatted manner.\n",
        "        print_results(results)\n",
        "        return False\n",
        "\n",
        "def extract_and_categorize_objects(bundle):\n",
        "    \"\"\"\n",
        "    Extracts objects from a bundle and categorizes them into SDO/SCO and SRO.\n",
        "    \"\"\"\n",
        "    sdo_sco_list = []\n",
        "    sro_list = []\n",
        "    objects = bundle.get(\"objects\", [])\n",
        "\n",
        "    for obj in objects:\n",
        "        obj_type = obj.get(\"type\", \"\")\n",
        "        if obj_type in [\"relationship\", \"sighting\", \"sighting-of\"]:\n",
        "            sro_list.append(obj)\n",
        "        else:\n",
        "            sdo_sco_list.append(obj)\n",
        "\n",
        "    return sdo_sco_list, sro_list"
      ],
      "metadata": {
        "id": "NZWNJTJHgO8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4**: Function for calculating metrics\n",
        "This is the main function for calculating metrics and averages.\n",
        "\n",
        "Three main metrics are calculated:\n",
        "\n",
        "*   **Precision**\n",
        "*   **Recall**\n",
        "*   **F1-Score**\n",
        "\n",
        "each at three levels of granularity:\n",
        "\n",
        "*   **Micro averaging**\n",
        "*   **Macro averaging**\n",
        "*   **Weighted averaging**\n",
        "\n",
        "Specifically:\n",
        "*   **Precision**: This metric answers the question: ‚ÄúOf all the STIX elements that my tool generated, what fraction was actually correct?‚Äù A high accuracy score indicates that the tool is reliable and does not generate much ‚Äúnoise‚Äù or incorrect information. It is a measure of output quality.\n",
        "*   **Recall**: This metric answers the question: ‚ÄúOf all the correct STIX elements that were present in the report, what fraction did my tool manage to find?‚Äù A high recall score indicates that the tool is comprehensive and does not omit much relevant information. It is a measure of coverage.\n",
        "*   **F1-Score**:  The F1-Score is the harmonic mean of Precision and Recall. It provides a single balanced score that takes both aspects into account. It is particularly useful when you want balanced performance, i.e., when it is equally important to minimize noise (high precision) and maximize coverage (high recall). The F1-Score heavily penalizes systems that excel in one metric at the expense of the other.\n",
        "*   **Micro Averaging**: Micro-averaging gives equal weight to each individual classification decision on an object instance. It answers the question: ‚ÄúConsidering all STIX objects extracted from all reports, what percentage of individual instances were handled correctly?‚Äù This score will be dominated by performance on the most numerous classes. If the tool is very good at extracting Indicators and IPv4 Addresses (which are very common), the Micro-F1 score will be high, even if performance on rare but important classes such as Campaign or Threat Actor is poor. In a multi-class context, micro-average Precision is equal to micro-average Recall, and therefore to the micro-average F1-Score and overall accuracy.\n",
        "*   **Macro Averaging**: Macro-averaging gives equal weight to each class, regardless of its frequency. It answers the question: ‚ÄúOn average, how does my tool perform on different types of CTI concepts?‚Äù This score treats the F1-Score for the Threat-Actor class (which may be based on only a few instances) as equally important as the F1-Score for the Indicator class class (based on hundreds of instances). It is a much more indicative measure of the tool's versatility and its ability to handle even the rarest and most difficult types of entities. If the tool performs poorly on a minority class, the Macro-F1 score will be significantly affected.\n",
        "*   **Weighted-Averaging**: This method represents a compromise between Micro and Macro. It attempts to take into account class imbalance (like Micro) while calculating metrics for each type (like Macro). In many scenarios, the Weighted-average result will be very close to that of Micro-average, as both end up giving more importance to classes with more instances."
      ],
      "metadata": {
        "id": "uKwEmnEjyX0f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Functions for Calculating and Printing Metrics\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def print_bundle_summary_metrics(results_dict, bundle_name):\n",
        "    \"\"\"\n",
        "    Calculate and print only the general metrics for a single bundle.\n",
        "    *** MODIFIED: Now returns the calculated metrics ***\n",
        "    \"\"\"\n",
        "    total_tp = sum(c['TP'] for c in results_dict.values())\n",
        "    total_fp = sum(c['FP'] for c in results_dict.values())\n",
        "    total_fn = sum(c['FN'] for c in results_dict.values())\n",
        "\n",
        "    precision_micro = total_tp / (total_tp + total_fp) if (total_tp + total_fp) > 0 else 0.0\n",
        "    recall_micro = total_tp / (total_tp + total_fn) if (total_tp + total_fn) > 0 else 0.0\n",
        "    f1_micro = 2 * (precision_micro * recall_micro) / (precision_micro + recall_micro) if (precision_micro + recall_micro) > 0 else 0.0\n",
        "\n",
        "    print(f\"\\n--- OVERALL METRICS FOR BUNDLE: {bundle_name} ---\")\n",
        "    print(f\"  - Overall Precision: {precision_micro:.2f}\")\n",
        "    print(f\"  - Overall Recall:    {recall_micro:.2f}\")\n",
        "    print(f\"  - Overall F1-Score:  {f1_micro:.2f}\")\n",
        "\n",
        "    # Return the calculated values\n",
        "    return precision_micro, recall_micro, f1_micro\n",
        "\n",
        "def calculate_and_print_metrics(total_results_per_object):\n",
        "    \"\"\"\n",
        "    Calculate and print the COMPLETE final report, including metrics by class\n",
        "    and aggregate averages.\n",
        "    *** MODIFIED TO RETURN THE METRICS DICTIONARIES FOR PLOTTING ***\n",
        "    \"\"\"\n",
        "    metrics_per_class = {}\n",
        "\n",
        "    # Calculation of metrics for each individual class of objects\n",
        "    for obj_type, counts in total_results_per_object.items():\n",
        "        tp = counts['TP']\n",
        "        fp = counts['FP']\n",
        "        fn = counts['FN']\n",
        "\n",
        "        # Precision\n",
        "        if (tp + fp) > 0:\n",
        "            precision = tp / (tp + fp)\n",
        "        else:\n",
        "            precision = 0.0\n",
        "\n",
        "        # Recall\n",
        "        if (tp + fn) > 0:\n",
        "            recall = tp / (tp + fn)\n",
        "        else:\n",
        "            recall = 0.0\n",
        "\n",
        "        # F1-Score\n",
        "        if (precision + recall) > 0:\n",
        "            f1_score = 2 * (precision * recall) / (precision + recall)\n",
        "        else:\n",
        "            f1_score = 0.0\n",
        "\n",
        "        # Support (number of actual instances for the class in the ground truth)\n",
        "        support = tp + fn\n",
        "\n",
        "        metrics_per_class[obj_type] = {\n",
        "            'Precision': precision,\n",
        "            'Recall': recall,\n",
        "            'F1-Score': f1_score,\n",
        "            'Support': support\n",
        "        }\n",
        "\n",
        "    # Calculation of averages\n",
        "    total_tp = sum(c['TP'] for c in total_results_per_object.values())\n",
        "    total_fp = sum(c['FP'] for c in total_results_per_object.values())\n",
        "    total_fn = sum(c['FN'] for c in total_results_per_object.values())\n",
        "    total_support = sum(m['Support'] for m in metrics_per_class.values())\n",
        "\n",
        "    # --- Micro Averages ---\n",
        "    if (total_tp + total_fp) > 0:\n",
        "        precision_micro = total_tp / (total_tp + total_fp)\n",
        "    else:\n",
        "        precision_micro = 0.0\n",
        "    if (total_tp + total_fn) > 0:\n",
        "        recall_micro = total_tp / (total_tp + total_fn)\n",
        "    else:\n",
        "        recall_micro = 0.0\n",
        "    if (precision_micro + recall_micro) > 0:\n",
        "        f1_micro = 2 * (precision_micro * recall_micro) / (precision_micro + recall_micro)\n",
        "    else:\n",
        "        f1_micro = 0.0\n",
        "\n",
        "    # --- Macro Average ---\n",
        "    num_classes = len(metrics_per_class)\n",
        "    if num_classes > 0:\n",
        "        precision_macro = sum(m['Precision'] for m in metrics_per_class.values()) / num_classes\n",
        "        recall_macro = sum(m['Recall'] for m in metrics_per_class.values()) / num_classes\n",
        "        f1_macro = sum(m['F1-Score'] for m in metrics_per_class.values()) / num_classes\n",
        "    else:\n",
        "        precision_macro, recall_macro, f1_macro = 0.0, 0.0, 0.0\n",
        "\n",
        "    # --- Weighted Average ---\n",
        "    if total_support > 0:\n",
        "        precision_weighted = sum(m['Precision'] * m['Support'] for m in metrics_per_class.values()) / total_support\n",
        "        recall_weighted = sum(m['Recall'] * m['Support'] for m in metrics_per_class.values()) / total_support\n",
        "        f1_weighted = sum(m['F1-Score'] * m['Support'] for m in metrics_per_class.values()) / total_support\n",
        "    else:\n",
        "        precision_weighted, recall_weighted, f1_weighted = 0.0, 0.0, 0.0\n",
        "\n",
        "    # *** NEW: Store aggregate metrics in a dictionary ***\n",
        "    aggregate_metrics = {\n",
        "        'MICRO AVG': {'Precision': precision_micro, 'Recall': recall_micro, 'F1-Score': f1_micro, 'Support': total_support},\n",
        "        'MACRO AVG': {'Precision': precision_macro, 'Recall': recall_macro, 'F1-Score': f1_macro, 'Support': total_support},\n",
        "        'WEIGHTED AVG': {'Precision': precision_weighted, 'Recall': recall_weighted, 'F1-Score': f1_weighted, 'Support': total_support}\n",
        "    }\n",
        "\n",
        "    # --- Printing results in a table (unchanged) ---\n",
        "    print(\"\\n\" + \"=\"*70); print(\"üìà PERFORMANCE METRICS REPORT üìà\".center(70)); print(\"=\"*70)\n",
        "    print(f\"| {'CLASS':<25} | {'PRECISION':>9} | {'RECALL':>9} | {'F1-SCORE':>9} | {'SUPPORT':>9} |\")\n",
        "    print(\"|\" + \"-\"*68 + \"|\")\n",
        "\n",
        "    for obj_type, metrics in sorted(metrics_per_class.items()):\n",
        "        p = metrics['Precision']\n",
        "        r = metrics['Recall']\n",
        "        f1 = metrics['F1-Score']\n",
        "        s = metrics['Support']\n",
        "\n",
        "        # *** QUESTA E' LA RIGA CORRETTA ***\n",
        "        print(f\"| {obj_type:<25} | {p:>8.2f} | {r:>8.2f} | {f1:>8.2f} | {s:>9} |\")\n",
        "\n",
        "    print(\"|\" + \"-\"*68 + \"|\")\n",
        "    print(\"|\" + \" \"*68 + \"|\")\n",
        "\n",
        "    print(f\"| {'MICRO AVG':<25} | {precision_micro:>8.2f} | {recall_micro:>8.2f} | {f1_micro:>8.2f} | {total_support:>9} |\")\n",
        "    print(f\"| {'MACRO AVG':<25} | {precision_macro:>8.2f} | {recall_macro:>8.2f} | {f1_macro:>8.2f} | {total_support:>9} |\")\n",
        "    print(f\"| {'WEIGHTED AVG':<25} | {precision_weighted:>8.2f} | {recall_weighted:>8.2f} | {f1_weighted:>8.2f} | {total_support:>9} |\")\n",
        "\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # *** NEW: Return the calculated data ***\n",
        "    return metrics_per_class, aggregate_metrics"
      ],
      "metadata": {
        "id": "01WjR0ReyoBn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 5**: Main Logic of Comparison\n",
        "This is the main cell that performs the entire process: it browses folders, loads bundles, compares them, and prints the results.\n",
        "\n",
        "The objects generated by the tool can belong to three categories:\n",
        "*   **TP** (**True Positive**): objects correctly identified by the tool that match the ground truth;\n",
        "*   **FP** (**False Positive**): objects that have been identified by the tool but are not part of the ground truth;\n",
        "*   **FN** (**False Negative**): objects that have not been identified by the tool but are part of the ground truth.\n",
        "\n",
        "Furthermore, TP, FP, and FN are also divided according to the type of object to which they belong in order to achieve a further level of granularity.\n",
        "\n",
        "SDOs/SCOs and SROs are compared in two different ways:\n",
        "*   SDOs/SCOs: for each object generated by the tool, the script searches for a\n",
        "semantically equivalent object in the ground truth objects using the function `object_equivalence(gen_obj, exp_obj, threshold=SIMILARITY_THRESHOLD)`, a match indicates that a **TP** has been found (the objects found are removed from their respective lists to avoid double counting). At the end of the comparison, all objects in the list generated by the tool represent an **FP**, while those in the ground truth list represent an **FN**.\n",
        "*   SRO: In this case, the comparison is made by comparing the **source object**, the **destination object**, and the **type of relationship**. Each is assigned a **weight** (40 and 40 for the two source and destination objects and 20 for the type of relationship). If the similarity threshold is set to 80, the two objects, source and destination, only need to be the same to obtain a similarity. The calculation of FP and FN is similar to the previous one.\n",
        "\n",
        "For indicator objects containing **YARA rules**, the comparison is customized. Specifically, the text contained in the pattern property is extracted and normalized, and then the two strings are compared.\n",
        "\n",
        "NOTE: All STIX metadata objects such as `reports` and `marking-definitions` are excluded from metric calculations."
      ],
      "metadata": {
        "id": "1pR8CfxGQMc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main_comparison_logic(generated_bundle_path, expert_bundle_path):\n",
        "    \"\"\"\n",
        "    Performs comparison using weighted logic for relationships.\n",
        "    Excludes indicators with non-STIX patterns, metadata objects and their associated relationships.\n",
        "    \"\"\"\n",
        "    METADATA_OBJECTS_TO_EXCLUDE = ['report', 'marking-definition']\n",
        "\n",
        "    # STEP 1: Validate the bundles before proceeding\n",
        "    if not validate_bundle(generated_bundle_path) or not validate_bundle(expert_bundle_path):\n",
        "        return {}  # <- Backslash rimosso\n",
        "    with open(generated_bundle_path, 'r') as f: generated_bundle_json = json.load(f) # <- Backslash rimosso\n",
        "    with open(expert_bundle_path, 'r') as f: expert_bundle_json = json.load(f) # <- Backslash rimosso\n",
        "\n",
        "    # STEP 2: Division of objects into lists\n",
        "    generated_sdo_sco, generated_sro = extract_and_categorize_objects(generated_bundle_json)\n",
        "    expert_sdo_sco, expert_sro = extract_and_categorize_objects(expert_bundle_json)\n",
        "\n",
        "    # STEP 3: Identify metadata objects and their IDs\n",
        "    metadata_objects_gen = [obj for obj in generated_sdo_sco if obj.get('type') in METADATA_OBJECTS_TO_EXCLUDE]\n",
        "    metadata_object_ids_gen = {obj['id'] for obj in metadata_objects_gen}\n",
        "\n",
        "    metadata_objects_exp = [obj for obj in expert_sdo_sco if obj.get('type') in METADATA_OBJECTS_TO_EXCLUDE]\n",
        "    metadata_object_ids_exp = {obj['id'] for obj in metadata_objects_exp}\n",
        "\n",
        "    # STEP 4: Isolation of special indicators (MODIFIED - NO LONGER EXCLUDES BY TYPE)\n",
        "    # We keep the structure but make the filters ineffective for pattern_type\n",
        "    special_indicators_gen = []\n",
        "    special_indicator_ids_gen = set()\n",
        "\n",
        "    special_indicators_exp = []\n",
        "    special_indicator_ids_exp = set()\n",
        "\n",
        "    print(f\"\\\\n‚ÑπÔ∏è  Nessun indicatore escluso in base al pattern_type.\")\n",
        "\n",
        "    # STEP 5: Create a unique set of all IDs to exclude\n",
        "    all_excluded_ids_gen = metadata_object_ids_gen.union(special_indicator_ids_gen)\n",
        "    all_excluded_ids_exp = metadata_object_ids_exp.union(special_indicator_ids_exp)\n",
        "\n",
        "    # STEP 6: Filtering all objects and relationships to be compared\n",
        "    sdo_sco_to_compare = [obj for obj in generated_sdo_sco if obj not in metadata_objects_gen and obj not in special_indicators_gen]\n",
        "    sdo_sco_to_compare_exp = [obj for obj in expert_sdo_sco if obj not in metadata_objects_exp and obj not in special_indicators_exp]\n",
        "\n",
        "    # STEP 7: Excludes relationships that connect to ANY excluded object\n",
        "    sro_to_compare = [rel for rel in generated_sro if rel.get('source_ref') not in all_excluded_ids_gen and rel.get('target_ref') not in all_excluded_ids_gen]\n",
        "    sro_to_compare_exp = [rel for rel in expert_sro if rel.get('source_ref') not in all_excluded_ids_exp and rel.get('target_ref') not in all_excluded_ids_exp]\n",
        "\n",
        "    # STEP 8: Initialization of the dictionary for TP, FP, and FN\n",
        "    results = defaultdict(lambda: {'TP': 0, 'FP': 0, 'FN': 0});\n",
        "    id_map = {}\n",
        "\n",
        "    unmatched_generated_sdo_sco = deepcopy(sdo_sco_to_compare)\n",
        "    search_pool_expert_sdo_sco = deepcopy(sdo_sco_to_compare_exp)\n",
        "    unmatched_generated_sro = deepcopy(sro_to_compare)\n",
        "    search_pool_expert_sro = deepcopy(sro_to_compare_exp)\n",
        "\n",
        "    print(f\"\\n--- Start comparison with similarity threshold >= {SIMILARITY_THRESHOLD} ---\")\n",
        "\n",
        "    # STEP 9: Comparison of all eligible SDOs/SCOs (REVISED LOGIC with Custom Indicator Comparison)\n",
        "    print(\"Comparing allowed SDO/SCO...\")\n",
        "\n",
        "    # Copies of the original lists (filtered) from which we will remove the elements\n",
        "    # These will ultimately be used to calculate FP and FN\n",
        "    unmatched_generated_sdo_sco = deepcopy(sdo_sco_to_compare)\n",
        "    search_pool_expert_sdo_sco = deepcopy(sdo_sco_to_compare_exp)\n",
        "\n",
        "    # Set of IDs of objects already matched to avoid double counting\n",
        "    matched_gen_ids = set()\n",
        "    matched_exp_ids = set()\n",
        "\n",
        "    # Helper function to normalise the content of YARA brackets\n",
        "    def normalize_braces_content(text):\n",
        "        def replacer(match):\n",
        "            content = match.group(1) # <- Backslash rimosso\n",
        "            # Remove special characters and convert to lowercase\n",
        "            cleaned_content = re.sub(r'[^a-z0-9\\\\s]', '', content.lower()) # <- Backslash rimosso\n",
        "            # Compress multiple spaces\n",
        "            normalized_content = ' '.join(cleaned_content.split()) # <- Backslash rimosso\n",
        "            return '{' + normalized_content + '}' # <- Backslash rimosso\n",
        "        # Apply the replacement to all occurrences of {...}\n",
        "        return re.sub(r'\\\\{(.*?)\\\\}', replacer, text, flags=re.DOTALL) # <- Backslash rimosso\n",
        "\n",
        "    for gen_obj in sdo_sco_to_compare:\n",
        "        if gen_obj['id'] in matched_gen_ids:\n",
        "            continue\n",
        "\n",
        "        best_match_expert_obj = None\n",
        "\n",
        "        for exp_obj in sdo_sco_to_compare_exp:\n",
        "            if exp_obj['id'] in matched_exp_ids:\n",
        "                continue\n",
        "\n",
        "            if gen_obj.get('type') == exp_obj.get('type'):\n",
        "                is_equivalent = False\n",
        "\n",
        "                # SPECIFIC COMPARISON LOGIC FOR INDICATORS\n",
        "                if gen_obj.get('type') == 'indicator':\n",
        "                    pattern_gen = gen_obj.get('pattern', '')\n",
        "                    pattern_exp = exp_obj.get('pattern', '')\n",
        "                    pattern_type_gen = gen_obj.get('pattern_type', 'stix')\n",
        "                    pattern_type_exp = exp_obj.get('pattern_type', 'stix')\n",
        "\n",
        "                    if pattern_type_gen == pattern_type_exp:\n",
        "                        if pattern_type_gen == 'stix':\n",
        "                            norm_gen = pattern_gen.strip().lower()\n",
        "                            norm_exp = pattern_exp.strip().lower()\n",
        "                            if norm_gen == norm_exp:\n",
        "                                is_equivalent = True\n",
        "                        elif pattern_type_gen == 'yara':\n",
        "                            # Customised normalisation for YARA\n",
        "                            # 1. Minuscule\n",
        "                            norm_gen = pattern_gen.lower()\n",
        "                            norm_exp = pattern_exp.lower()\n",
        "                            # 2. Remove comments //...\n",
        "                            norm_gen = re.sub(r'//.*', '', norm_gen)\n",
        "                            norm_exp = re.sub(r'//.*', '', norm_exp)\n",
        "                            # 3. Normalise whitespace -> single space\n",
        "                            norm_gen = ' '.join(norm_gen.split())\n",
        "                            norm_exp = ' '.join(norm_exp.split())\n",
        "\n",
        "                            norm_gen_braces = normalize_braces_content(norm_gen)\n",
        "                            norm_exp_braces = normalize_braces_content(norm_exp)\n",
        "                            if norm_gen_braces == norm_exp_braces:\n",
        "                                is_equivalent = True\n",
        "\n",
        "                        # Default handling for other pattern_type\n",
        "                        else:\n",
        "                             norm_gen = pattern_gen.strip().lower()\n",
        "                             norm_exp = pattern_exp.strip().lower()\n",
        "                             if norm_gen == norm_exp:\n",
        "                                is_equivalent = True\n",
        "\n",
        "                else:\n",
        "                    try:\n",
        "                        if object_equivalence(gen_obj, exp_obj, threshold=SIMILARITY_THRESHOLD):\n",
        "                            is_equivalent = True\n",
        "                    except Exception as e:\n",
        "                         pass\n",
        "\n",
        "                # If you find a valid match, record the IDs and exit the internal loop.\n",
        "                if is_equivalent:\n",
        "                    obj_type = gen_obj['type']\n",
        "                    results[obj_type]['TP'] += 1\n",
        "                    id_map[gen_obj['id']] = exp_obj['id']\n",
        "                    matched_gen_ids.add(gen_obj['id'])\n",
        "                    matched_exp_ids.add(exp_obj['id'])\n",
        "                    best_match_expert_obj = exp_obj\n",
        "                    break\n",
        "\n",
        "    # After the cycles, the 'unmatched_generated_sdo_sco' and 'search_pool_expert_sdo_sco' lists\n",
        "    # still contain all the initial objects. Now we recalculate them correctly.\n",
        "    final_unmatched_gen = [obj for obj in sdo_sco_to_compare if obj['id'] not in matched_gen_ids]\n",
        "    final_unmatched_exp = [obj for obj in sdo_sco_to_compare_exp if obj['id'] not in matched_exp_ids]\n",
        "\n",
        "    # Update the lists that will be used later (for SRO and final FP/FN calculation)\n",
        "    # Overwrite the original lists defined in STEP 8 with the newly calculated ones\n",
        "    unmatched_generated_sdo_sco = final_unmatched_gen\n",
        "    search_pool_expert_sdo_sco = final_unmatched_exp\n",
        "\n",
        "    # ======================= STEP 10: NEW LOGIC OF COMPARISON FOR RELATIONSHIPS =======================\n",
        "    print(\"Esecuzione del confronto PONDERATO per SRO consentito...\")\n",
        "\n",
        "    # Let's define the weights for each component of the relationship\n",
        "    weights = {'source_ref': 40, 'target_ref': 40, 'relationship_type': 20}\n",
        "\n",
        "    for gen_rel in sro_to_compare:\n",
        "        best_match_expert_rel = None\n",
        "        highest_score = -1\n",
        "\n",
        "        for exp_rel in search_pool_expert_sro:\n",
        "            current_score = 0\n",
        "            # Compare ‚Äòsource_ref‚Äô using id_map\n",
        "            if id_map.get(gen_rel.get('source_ref')) == exp_rel.get('source_ref'):\n",
        "                current_score += weights['source_ref']\n",
        "\n",
        "            # Compare ‚Äòtarget_ref‚Äô using id_map\n",
        "            if id_map.get(gen_rel.get('target_ref')) == exp_rel.get('target_ref'):\n",
        "                current_score += weights['target_ref']\n",
        "\n",
        "            # Compare 'tipo_relazione'\n",
        "            if gen_rel.get('relationship_type') == exp_rel.get('relationship_type'):\n",
        "                current_score += weights['relationship_type']\n",
        "\n",
        "            if current_score > highest_score:\n",
        "                highest_score = current_score\n",
        "                best_match_expert_rel = exp_rel\n",
        "\n",
        "        # Check whether the best score exceeds the threshold\n",
        "        if highest_score >= RELATIONSHIP_SIMILARITY_THRESHOLD:\n",
        "            results['relationship']['TP'] += 1\n",
        "            # Removes matched items so they don't need to be checked again\n",
        "            unmatched_generated_sro.remove(gen_rel)\n",
        "            if best_match_expert_rel in search_pool_expert_sro:\n",
        "                 search_pool_expert_sro.remove(best_match_expert_rel)\n",
        "\n",
        "    # ================================= END OF NEW LOGIC ==================================\n",
        "\n",
        "    # STEP 11: Calculation of FP and FN\n",
        "    print(\"Step 3: Final calculation of FP and FN...\")\n",
        "    for unmatched_obj in unmatched_generated_sdo_sco + unmatched_generated_sro:\n",
        "        results[unmatched_obj['type']]['FP'] += 1\n",
        "    for unmatched_obj in search_pool_expert_sdo_sco + search_pool_expert_sro:\n",
        "        results[unmatched_obj['type']]['FN'] += 1\n",
        "\n",
        "    print(\"\\\\n--- Calculation Results (TP, FP, FN) ---\");\n",
        "    print(json.dumps(dict(results), indent=2))\n",
        "    return dict(results)\n",
        "\n",
        "# SCRIPT EXECUTION\n",
        "\n",
        "# Initialize the variables before starting the loop.\n",
        "total_results_per_object = defaultdict(lambda: {'TP': 0, 'FP': 0, 'FN': 0})\n",
        "grand_totals = {'TP': 0, 'FP': 0, 'FN': 0}\n",
        "\n",
        "# *** NEW: Initialize list to store metrics for each bundle ***\n",
        "all_bundle_metrics = []\n",
        "\n",
        "\n",
        "if not os.path.exists(BASE_DRIVE_DIR):\n",
        "    print(f\"‚ùå ERROR: The specified directory does not exist: {BASE_DRIVE_DIR}\")\n",
        "else:\n",
        "    for dirname in sorted(os.listdir(BASE_DRIVE_DIR)):\n",
        "        dirpath = os.path.join(BASE_DRIVE_DIR, dirname)\n",
        "        if os.path.isdir(dirpath):\n",
        "            print(f\"\\\\n{'='*15} Analyzing the folder: {dirname} {'='*15}\")\n",
        "            ground_truth_path, predicted_path = None, None\n",
        "            try:\n",
        "                for filename in os.listdir(dirpath):\n",
        "                    if filename.endswith(\"_gt.json\"):\n",
        "                        ground_truth_path = os.path.join(dirpath, filename)\n",
        "                    elif filename.endswith(\"_pred.json\"):\n",
        "                        predicted_path = os.path.join(dirpath, filename)\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Unable to access the folder '{dirname}'.\")\n",
        "                continue\n",
        "\n",
        "            if ground_truth_path and predicted_path:\n",
        "                print(f\"Ground Truth found: {os.path.basename(ground_truth_path)}\")\n",
        "                print(f\"Found Expected Bundle: {os.path.basename(predicted_path)}\")\n",
        "\n",
        "                # Call the function and save the result of the single bundle\n",
        "                bundle_results = main_comparison_logic(predicted_path, ground_truth_path)\n",
        "\n",
        "                if bundle_results:\n",
        "                    # *** NEW: Calculate, print, AND capture metrics for the SINGLE bundle ***\n",
        "                    p_micro, r_micro, f1_micro = print_bundle_summary_metrics(bundle_results, dirname)\n",
        "\n",
        "                    # *** NEW: Add captured metrics to our list ***\n",
        "                    all_bundle_metrics.append({\n",
        "                        'bundle': dirname,\n",
        "                        'Precision': p_micro,\n",
        "                        'Recall': r_micro,\n",
        "                        'F1-Score': f1_micro\n",
        "                    })\n",
        "\n",
        "                    # Update totals by object type\n",
        "                    for obj_type, counts in bundle_results.items():\n",
        "                        total_results_per_object[obj_type]['TP'] += counts['TP']\n",
        "                        total_results_per_object[obj_type]['FP'] += counts['FP']\n",
        "                        total_results_per_object[obj_type]['FN'] += counts['FN']\n",
        "            else:\n",
        "                print(f\"Bundle pair not found.\")\n",
        "\n",
        "# Calculate the grand total by adding up the totals for each type of item.\n",
        "for obj_type, counts in total_results_per_object.items():\n",
        "    grand_totals['TP'] += counts['TP']\n",
        "    grand_totals['FP'] += counts['FP']\n",
        "    grand_totals['FN'] += counts['FN']\n",
        "\n",
        "# --- Printing Final Results ---\n",
        "\n",
        "# Print detailed totals by object type (raw data)\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"üìä TOTAL RESULTS PER STIX OBJECT TYPE (RAW DATA) üìä\")\n",
        "print(\"=\"*50)\n",
        "print(json.dumps(total_results_per_object, indent=2))\n",
        "\n",
        "# Print the total (raw data)\n",
        "print(\"\\\\n\" + \"=\"*50)\n",
        "print(\"üèÜ GRAND TOTAL (ALL OBJECTS) (RAW DATA) üèÜ\")\n",
        "print(\"=\"*50)\n",
        "print(json.dumps(grand_totals, indent=2))\n",
        "\n",
        "# Call the function to calculate, print, and return metrics.\n",
        "metrics_per_class, aggregate_metrics = calculate_and_print_metrics(total_results_per_object)\n",
        "\n",
        "print(\"\\n\\n‚úÖ Dati metrici catturati e pronti per la visualizzazione.\")"
      ],
      "metadata": {
        "id": "h_yrwh6USmv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 6**: Generation of analysis charts\n",
        "\n",
        "Creating graphs for statistical analysis:\n",
        "\n",
        "\n",
        "\n",
        "*   **Aggregate metrics chart:** Shows the overall performance of the model according to the three different methods of calculating the average.\n",
        "*   **Performance chart by STIX object type:** It shows in detail how the model performs on each STIX object class, allowing you to identify strengths and weaknesses.\n",
        "*   **Distribution analysis chart:** It shows how unbalanced the dataset is: there are many instances of some classes and very few of others.\n",
        "*   **Error analysis chart:** This chart uses raw data to show the composition of ‚Äúsuccesses‚Äù and ‚Äúerrors‚Äù for each class.\n",
        "*   **Performance variability chart by bundle:** Analyze how consistent the generator is by analyzing the metrics of each individual bundle, showing the variability between the different types of reports.\n",
        "\n",
        "\n",
        "Save the results to an output folder.\n"
      ],
      "metadata": {
        "id": "XaP7Fe2KvElX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import json\n",
        "\n",
        "print(\"--- Avvio Creazione Grafici e Salvataggio Dati ---\")\n",
        "\n",
        "# --- 1. IMPOSTAZIONE AMBIENTE ---\n",
        "\n",
        "# Definisci la cartella di output (verr√† creata se non esiste)\n",
        "OUTPUT_DIR = os.path.join(BASE_DRIVE_DIR, \"Risultati_Grafici_Valutazione\")\n",
        "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "print(f\"Tutti i file verranno salvati in: {OUTPUT_DIR}\")\n",
        "\n",
        "# --- 2. PREPARAZIONE DATI ---\n",
        "\n",
        "# Converte i dizionari delle metriche in DataFrames Pandas\n",
        "# Dati grezzi TP/FP/FN\n",
        "df_raw_counts = pd.DataFrame.from_dict(total_results_per_object, orient='index').fillna(0).astype(int)\n",
        "df_raw_counts = df_raw_counts.reindex(columns=['TP', 'FP', 'FN'], fill_value=0) # Assicura colonne\n",
        "\n",
        "# Dati metriche per classe\n",
        "df_metrics_class = pd.DataFrame.from_dict(metrics_per_class, orient='index')\n",
        "\n",
        "# Dati metriche aggregate\n",
        "df_metrics_agg = pd.DataFrame.from_dict(aggregate_metrics, orient='index')\n",
        "\n",
        "# *** NEW: Dati metriche per singolo bundle ***\n",
        "df_bundle_metrics = pd.DataFrame(all_bundle_metrics)\n",
        "\n",
        "\n",
        "# --- 3. CREAZIONE E SALVATAGGIO GRAFICI ---\n",
        "\n",
        "# --- GRAFICO 1: Metriche Aggregate (Micro, Macro, Weighted) ---\n",
        "try:\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    df_metrics_agg[['Precision', 'Recall', 'F1-Score']].plot(kind='bar', rot=0)\n",
        "    plt.title('Performance Aggregate (Micro, Macro, Weighted)')\n",
        "    plt.ylabel('Punteggio')\n",
        "    plt.xlabel('Tipo di Media')\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.legend(loc='lower right')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plot_path_1 = os.path.join(OUTPUT_DIR, \"1_grafico_metriche_aggregate.png\")\n",
        "    plt.savefig(plot_path_1)\n",
        "    plt.show()\n",
        "    print(f\"‚úÖ Grafico 1 salvato: 1_grafico_metriche_aggregate.png\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Errore creazione Grafico 1: {e}\")\n",
        "\n",
        "# --- GRAFICO 2: F1-Score per Tipo di Oggetto (solo con Support > 0) ---\n",
        "try:\n",
        "    df_metrics_class_filtered = df_metrics_class[df_metrics_class['Support'] > 0].sort_values('F1-Score', ascending=False)\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    df_metrics_class_filtered[['Precision', 'Recall', 'F1-Score']].plot(kind='bar')\n",
        "    plt.title('Performance per Tipo di Oggetto STIX (con Support > 0)')\n",
        "    plt.ylabel('Punteggio')\n",
        "    plt.xlabel('Tipo Oggetto STIX')\n",
        "    plt.ylim(0, 1.0)\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.tight_layout()\n",
        "    plot_path_2 = os.path.join(OUTPUT_DIR, \"2_grafico_performance_per_classe.png\")\n",
        "    plt.savefig(plot_path_2)\n",
        "    plt.show()\n",
        "    print(f\"‚úÖ Grafico 2 salvato: 2_grafico_performance_per_classe.png\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Errore creazione Grafico 2: {e}\")\n",
        "\n",
        "# --- GRAFICO 3: Distribuzione del Supporto (Conteggio istanze Ground Truth) ---\n",
        "try:\n",
        "    df_support = df_metrics_class[df_metrics_class['Support'] > 0]['Support'].sort_values(ascending=False)\n",
        "    plt.figure(figsize=(15, 8))\n",
        "    df_support.plot(kind='bar', color='skyblue')\n",
        "    plt.title('Distribuzione del Supporto per Classe (N. Istanze nel Ground Truth)')\n",
        "    plt.ylabel('Numero di Istanze (Support)')\n",
        "    plt.xlabel('Tipo Oggetto STIX')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plot_path_3 = os.path.join(OUTPUT_DIR, \"3_grafico_distribuzione_supporto.png\")\n",
        "    plt.savefig(plot_path_3)\n",
        "    plt.show()\n",
        "    print(f\"‚úÖ Grafico 3 salvato: 3_grafico_distribuzione_supporto.png\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Errore creazione Grafico 3: {e}\")\n",
        "\n",
        "# --- GRAFICO 4: Analisi Errori (TP, FP, FN) per Classe ---\n",
        "try:\n",
        "    # Filtra solo classi con almeno un TP, FP, o FN per evitare grafici vuoti\n",
        "    df_raw_filtered = df_raw_counts[(df_raw_counts['TP'] > 0) | (df_raw_counts['FP'] > 0) | (df_raw_counts['FN'] > 0)]\n",
        "    df_raw_filtered = df_raw_filtered.sort_values(['FN', 'FP'], ascending=[False, False])\n",
        "\n",
        "    plt.figure(figsize=(15, 10))\n",
        "    df_raw_filtered.plot(kind='bar', stacked=True, figsize=(15, 8), colormap='viridis')\n",
        "    plt.title('Analisi Errori: True Positives (TP), False Positives (FP), False Negatives (FN)')\n",
        "    plt.ylabel('Conteggio Oggetti')\n",
        "    plt.xlabel('Tipo Oggetto STIX')\n",
        "    plt.legend(loc='upper right')\n",
        "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "    plt.tight_layout()\n",
        "    plot_path_4 = os.path.join(OUTPUT_DIR, \"4_grafico_analisi_errori_tp_fp_fn.png\")\n",
        "    plt.savefig(plot_path_4)\n",
        "    plt.show()\n",
        "    print(f\"‚úÖ Grafico 4 salvato: 4_grafico_analisi_errori_tp_fp_fn.png\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Errore creazione Grafico 4: {e}\")\n",
        "\n",
        "# --- *** NEW: GRAFICO 5: Variabilit√† Performance per Bundle *** ---\n",
        "try:\n",
        "    if not df_bundle_metrics.empty:\n",
        "        df_bundle_metrics_sorted = df_bundle_metrics.sort_values('F1-Score', ascending=False)\n",
        "        plt.figure(figsize=(15, 8))\n",
        "        df_bundle_metrics_sorted.plot(kind='bar', x='bundle', y='F1-Score', color='coral')\n",
        "        plt.title('Variabilit√† Performance (F1-Score) per Bundle')\n",
        "        plt.ylabel('F1-Score (Micro Avg)')\n",
        "        plt.xlabel('Bundle')\n",
        "        plt.ylim(0, 1.0)\n",
        "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
        "        plt.tight_layout()\n",
        "        plot_path_5 = os.path.join(OUTPUT_DIR, \"5_grafico_variabilita_per_bundle.png\")\n",
        "        plt.savefig(plot_path_5)\n",
        "        plt.show()\n",
        "        print(f\"‚úÖ Grafico 5 salvato: 5_grafico_variabilita_per_bundle.png\")\n",
        "    else:\n",
        "        print(\"‚ÑπÔ∏è Grafico 5 saltato: nessun dato sui bundle individuali √® stato catturato.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Errore creazione Grafico 5: {e}\")\n",
        "\n",
        "\n",
        "# --- 4. SALVATAGGIO DATI GREZZI ---\n",
        "\n",
        "try:\n",
        "    # Salva i CSV\n",
        "    df_metrics_class.to_csv(os.path.join(OUTPUT_DIR, \"report_metriche_per_classe.csv\"))\n",
        "    df_metrics_agg.to_csv(os.path.join(OUTPUT_DIR, \"report_metriche_aggregate.csv\"))\n",
        "    df_raw_counts.to_csv(os.path.join(OUTPUT_DIR, \"report_conteggi_grezzi_tp_fp_fn.csv\"))\n",
        "    # *** NEW: Salva CSV metriche bundle ***\n",
        "    df_bundle_metrics.to_csv(os.path.join(OUTPUT_DIR, \"report_metriche_per_bundle.csv\"), index=False)\n",
        "\n",
        "    # Salva i JSON originali\n",
        "    with open(os.path.join(OUTPUT_DIR, \"dati_totali_per_oggetto.json\"), 'w') as f:\n",
        "        json.dump(total_results_per_object, f, indent=2)\n",
        "\n",
        "    with open(os.path.join(OUTPUT_DIR, \"dati_grand_total.json\"), 'w') as f:\n",
        "        json.dump(grand_totals, f, indent=2)\n",
        "\n",
        "    print(f\"‚úÖ Tutti i file di dati (CSV e JSON) sono stati salvati.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Errore durante il salvataggio dei file di dati: {e}\")\n",
        "\n",
        "print(\"--- Processo completato. ---\")"
      ],
      "metadata": {
        "id": "VyFnMcNr1Yb4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}