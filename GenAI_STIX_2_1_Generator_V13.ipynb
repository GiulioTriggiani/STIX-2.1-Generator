{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BL4sy90XFI4y"
      },
      "source": [
        "# **GenAI-Powered STIX 2.1 Generator**\n",
        "\n",
        "**Notebook Version:** 13.0  \n",
        "**Author:** Antonio Formato  \n",
        "**Python Version:** >= 3.8  \n",
        "**Key Libraries:** `stix2`, `openai`, `iocextract`, `MarkItDown`\n",
        "\n",
        "---\n",
        "\n",
        "## **Objective**\n",
        "\n",
        "This notebook automates the conversion of unstructured Cyber Threat Intelligence (CTI) reports into structured, machine-readable STIX 2.1 bundles. By leveraging a Large Language Model (LLM) for entity and relationship extraction, it streamlines the intelligence lifecycle, enabling faster integration with security platforms like TIPs, SIEMs, and SOARs.\n",
        "\n",
        "## **Workflow Overview**\n",
        "\n",
        "1. **Setup**: Configure the environment.\n",
        "2. **Data Ingestion**: Load the raw CTI report text in various ways.\n",
        "3. **Advanced IOC Extraction**: Use a hybrid regex and LLM approach to identify and validate all Indicators of Compromise (IOCs).\n",
        "4. **Programmatic Object Generation** (**SCOs & Indicators**): Convert the validated IOCs into STIX Cyber Observable Objects (SCOs) and create corresponding `Indicator` objects with `derived-from` relationships.\n",
        "5. **Comprehensive Entity Extraction** (**SDOs**): Employ an LLM to parse the entire report for high-level STIX Domain Objects (SDOs) like `Malware`, `Attack Pattern`, and `Identity`.\n",
        "6. **Final Bundling**: Assemble all extracted STIX objects (SDOs, SCOs, SROs) into a single, cohesive, and contextually rich STIX 2.1 Bundle and save it to a JSON file.\n",
        "7. **Populating GitHub repo**: Automatically populating a public GitHub repository with the generated STIX bundles.\n",
        "8. **STIX Viewer**: STIX 2.1 bundle Visualizer."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yB80ixjOw0ve"
      },
      "source": [
        "## **Part 1**: Setup\n",
        "\n",
        "This initial block handles all the necessary setup for the notebook.\n",
        "\n",
        "Especially sets up the Azure OpenAI client using credentials stored securely in the environment.\n",
        "\n",
        "It is also possible to configure some variables related to the LLM model used, specifically:\n",
        "*   **Temperature**: Controls the level of creativity or predictability in the model's responses. A low temperature (0.1) makes the model more predictable and deterministic, while a high temperature (1.0) makes the model more creative.\n",
        "*   **Reasoning**: Controls the amount of “thinking” or analysis that the model devotes to your request before generating a response. It can be set to four values: minimal, low, medium, or high. With a low (minimal) value, the model will respond faster, but may be less accurate, less contextual, or unable to follow complex instructions. With a high value, the model will take longer to analyze the request, evaluate different “chains of thought,” and produce a more accurate, detailed response that is faithful to the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E5luqesZsxLG"
      },
      "outputs": [],
      "source": [
        "# Initialization\n",
        "!pip install openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d4orvzx-txTf"
      },
      "outputs": [],
      "source": [
        "from openai import AzureOpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "# Configuration: Set up the Azure OpenAI client\n",
        "try:\n",
        "    client = AzureOpenAI(\n",
        "      azure_endpoint = userdata.get('AZURE_OPENAI_ENDPOINT'),\n",
        "      api_key=userdata.get('AZURE_OPENAI_KEY'),\n",
        "      api_version=\"2024-12-01-preview\"\n",
        "    )\n",
        "    DEPLOYMENT_NAME = userdata.get('DEPLOYMENT_NAME')\n",
        "    print(\"✅ Azure OpenAI client configured successfully.\")\n",
        "\n",
        "    # --- GLOBAL CONFIGURATION OF MODELS ---\n",
        "    # Change these values to control all API calls\n",
        "    # The temperature can be a value between 0.1 and 1, lower temperature for more predictable, structured output.\n",
        "    # Reasoning can be set to: minimal, low, medium, or high values.\n",
        "\n",
        "    # Part 3 (IOC extraction)\n",
        "    TEMP_IOC_EXTRACTION = 1\n",
        "    REASONING_IOC_EXTRACTION = \"high\"\n",
        "\n",
        "    # Part 5 (SDO extraction)\n",
        "    TEMP_SDO_EXTRACTION = 1\n",
        "    REASONING_SDO_EXTRACTION = \"high\"\n",
        "\n",
        "    print(\"✅ Global configuration variables loaded.\")\n",
        "    # --- END OF CONFIGURATION ---\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"❌ Error configuring Azure OpenAI client: {e}\")\n",
        "    print(\"Please ensure AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_KEY, and DEPLOYMENT_NAME are set in Colab secrets.\")\n",
        "    client = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXfFY7ddBg87"
      },
      "source": [
        "## **Part 2**: Data Ingestion\n",
        "\n",
        "These blocks implement the ability to use various types of input to interact with the notebook:\n",
        "\n",
        "*   **Raw input**: allows you to paste the text of a CTI report directly into the notebook;\n",
        "*   **Web Scraper**: allows you to enter a blog post writeup URL and converts it into text for the notebook;\n",
        "*   **Markdown to PDF converter**: allows you to retrieve a PDF file from the drive you entered and use it as input for the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IWRCXLbHw6IS"
      },
      "source": [
        "### **Raw input**\n",
        "\n",
        "This block allows you to insert the raw text of a CTI report directly into the notebook for analysis.\n",
        "\n",
        "Once you have entered all the necessary text, type “END” or “end” on a new line and press enter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPxPAcJOv5Dm"
      },
      "outputs": [],
      "source": [
        "# Data Ingestion: Paste text directly via interactive input\n",
        "print(\"\\nPaste your report content below. When you are done, type 'END' on a new line and press Enter.\")\n",
        "lines = []\n",
        "while True:\n",
        "    try:\n",
        "        line = input()\n",
        "        if line.strip().upper() == 'END':\n",
        "            break\n",
        "        lines.append(line)\n",
        "    except EOFError:\n",
        "        break\n",
        "text = \"\\n\".join(lines)\n",
        "\n",
        "if text and text.strip():\n",
        "    print(f\"\\n✅ Successfully loaded {len(text)} characters from pasted text.\")\n",
        "else:\n",
        "    print(\"\\n⚠️ No text was pasted or an error occurred.\")\n",
        "    text = None"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWNOvrqge0o4"
      },
      "source": [
        "### **Web Scraper**\n",
        "\n",
        "This section implements a web scraper to extract text content from a specified blog post URL (infosec writeup).\n",
        "\n",
        "Utilizing the requests library and BeautifulSoup (bs4), the code ensures successful retrieval of data before proceeding with the extraction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EF7lokcFe2bv"
      },
      "outputs": [],
      "source": [
        "# Initialization\n",
        "!pip install requests\n",
        "!pip install beautifulsoup4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mbV_zjOkfgDj"
      },
      "outputs": [],
      "source": [
        "#Web scraper\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "def scrape_text(url):\n",
        "  # Add user-agent to avoid issue when scrapping most website\n",
        "  headers = {\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"}\n",
        "\n",
        "  # Send a GET request to the URL\n",
        "  response = requests.get(url, headers=headers)\n",
        "\n",
        "  # If the GET request is successful, the status code will be 200\n",
        "  if response.status_code == 200:\n",
        "    # Get the content of the response\n",
        "    page_content = response.content\n",
        "    # Create a BeautifulSoup object and specify the parser\n",
        "    soup = BeautifulSoup(page_content, \"html.parser\")\n",
        "    # Get the text of the soup object\n",
        "    text = soup.get_text()\n",
        "    # Return the text\n",
        "    return text\n",
        "  else:\n",
        "    return \"Failed to scrape the website\"\n",
        "\n",
        "# Enter site\n",
        "print(\"Enter blog post writeup url and press Enter\")\n",
        "url = input()\n",
        "\n",
        "text = scrape_text(url)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WGIokr9CkQuF"
      },
      "source": [
        "### **PDF to Markdown Converter**\n",
        "\n",
        "This section implements a PDF to Markdown converter.\n",
        "\n",
        "Use the MarkItDown tool to perform the conversion. You can enter the name of one of the files in the PDF_Reports folder.\n",
        "\n",
        "The conversion result will be saved in the Markdown_Reports folder.\n",
        "\n",
        "**Note**: the first time requires connection to Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TeYfzgAtkS0I"
      },
      "outputs": [],
      "source": [
        "# install MarkItDown\n",
        "!pip install 'markitdown[all]'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld3SrOYwkb8o"
      },
      "outputs": [],
      "source": [
        "# Importing the necessary libraries\n",
        "import os\n",
        "from google.colab import drive\n",
        "# Import the MarkItDown class from the library\n",
        "from markitdown import MarkItDown\n",
        "\n",
        "# Mount Google Drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "def convert_pdf_to_markdown(filename, folder_path):\n",
        "    \"\"\"\n",
        "    Converts a PDF file specified from Google Drive to Markdown format using\n",
        "    the MarkItDown class.\n",
        "\n",
        "    Args:\n",
        "        filename (str): The name of the PDF file to be converted (e.g., “report.pdf”).\n",
        "        folder_path (str): The path to the folder on Google Drive that contains the PDFs.\n",
        "\n",
        "    Returns:\n",
        "        str: The content of the PDF converted to Markdown, or an empty string in case of error.\n",
        "    \"\"\"\n",
        "    # Function that combines a folder path and a file name\n",
        "    full_pdf_path = os.path.join(folder_path, filename)\n",
        "\n",
        "    # Verify that the file exists in the specified path\n",
        "    if not os.path.exists(full_pdf_path):\n",
        "        print(f\"ERROR: The file '{filename}' was not found in the path '{folder_path}'.\")\n",
        "        print(\"Check the file name and make sure it is in the correct folder.\")\n",
        "        return \"\"\n",
        "\n",
        "    # Proceed with the conversion\n",
        "    print(f\"File '{filename}' found. I'm starting the conversion to Markdown...\")\n",
        "    try:\n",
        "        # 1. Create an instance of the MarkItDown class\n",
        "        md_converter = MarkItDown(enable_plugins=False)\n",
        "\n",
        "        # 2. CCall the .convert() method on the created object\n",
        "        result = md_converter.convert(full_pdf_path)\n",
        "\n",
        "        print(\"Conversion successfully completed!\")\n",
        "\n",
        "        # 3. Return the textual content of the result\n",
        "        return result.text_content\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred while converting the PDF.: {e}\")\n",
        "        return \"\"\n",
        "\n",
        "# Define the folder path on Drive\n",
        "# MAKE SURE YOU HAVE A FOLDER CALLED “PDF_Reports” IN YOUR DRIVE\n",
        "drive_folder = '/content/drive/MyDrive/Reports/PDF_Reports/'\n",
        "\n",
        "# Ask the user to enter the name of the PDF file (with the extension .pdf)\n",
        "pdf_input_name = input(f\"Enter the name of the PDF file (e.g., test.pdf) located in '{drive_folder}': \")\n",
        "\n",
        "# Call the function and save the result in the variable 'text'\n",
        "text = convert_pdf_to_markdown(pdf_input_name, drive_folder)\n",
        "\n",
        "# Check the result, print the preview, and save the .md file.\n",
        "if text:\n",
        "    print(\"\\n--- Preview of the extracted text (first 500 characters) ---\")\n",
        "    print(text[:500] + \"...\")\n",
        "\n",
        "    # Define the destination folder path for Markdown files\n",
        "    markdown_folder = '/content/drive/MyDrive/Reports/Markdown_Reports/'\n",
        "\n",
        "    # Create the folder if it doesn't exist\n",
        "    os.makedirs(markdown_folder, exist_ok=True)\n",
        "\n",
        "    # Create the name for the .md file based on the name of the original PDF.\n",
        "    base_filename = os.path.splitext(pdf_input_name)[0]\n",
        "    markdown_filename = f\"{base_filename}.md\"\n",
        "\n",
        "    #Create the full path by combining the folder and file name.\n",
        "    full_save_path = os.path.join(markdown_folder, markdown_filename)\n",
        "\n",
        "    # Write the contents of the ‘text’ variable to the new file\n",
        "    with open(full_save_path, 'w', encoding='utf-8') as file:\n",
        "        file.write(text)\n",
        "\n",
        "    print(\"\\n--- Saving Markdown File ---\")\n",
        "    print(f\"Markdown file successfully saved in: {full_save_path}\")\n",
        "else:\n",
        "    print(\"\\nThe variable ‘text’ is empty due to a previous error.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZq9sSahfFwX"
      },
      "source": [
        "## **Part 3**: Advanced IOC Extraction\n",
        "\n",
        "This block implements a robust, three-stage pipeline to identify, validate, and score Indicators of Compromise (IOCs) from the source text.\n",
        "\n",
        "* **Stage 1** (**Regex Triage**): Performs a quick first pass using `iocextract` to find common IOC patterns like IPs and hashes.\n",
        "* **Stage 2** (**LLM Analysis**): Uses the LLM with a specific function-calling schema to perform a deep contextual analysis, validating that the candidates are genuinely malicious and extracting their associated names and descriptions.\n",
        "* **Stage 3** (**Consolidation**): Merges the results from the first two stages, using the regex findings to increase the confidence score of the LLM-validated IOCs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U5ChCa9pLDde"
      },
      "outputs": [],
      "source": [
        "# Initialization\n",
        "!pip install stix2\n",
        "!pip install iocextract"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hSLLNUF1fMFQ"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "import re\n",
        "import uuid\n",
        "from typing import List, Dict, Any\n",
        "from datetime import datetime\n",
        "import iocextract\n",
        "from stix2 import (Indicator, Malware, Tool, AttackPattern, Infrastructure,\n",
        "                   Relationship, Bundle, File, IPv4Address, Directory, DomainName, WindowsRegistryKey,\n",
        "                   ThreatActor, Vulnerability, Identity)\n",
        "\n",
        "# Schema Definition for LLM Function Calling\n",
        "IOC_FUNCTION_SCHEMA = {\n",
        "    'name': 'extract_and_validate_iocs',\n",
        "    'description': 'Extracts and validates IOCs from a CTI report.',\n",
        "    'parameters': {\n",
        "        'type': 'object',\n",
        "        'properties': {\n",
        "            'iocs': {\n",
        "                'type': 'array',\n",
        "                'description': 'A list of validated IOCs found in the text.',\n",
        "                'items': {\n",
        "                    'type': 'object',\n",
        "                    'properties': {\n",
        "                        'value': {'type': 'string', 'description': \"The normalized IOC value (e.g., IP address, domain, full URL, hash, directory path, registry key). For composite file paths, this might be the full path string.\"},\n",
        "                        'type': {'type': 'string', 'description': \"The specific type of IOC. Use STIX compatible types like 'ipv4', 'domain-name', 'url', 'md5', 'sha1', 'sha256', 'mutex', 'windows-registry-key', 'directory'. For file paths involving both directory and filename, use the special type 'file-path'.\"},\n",
        "                        'name': {'type': 'string', 'description': \"A short, descriptive name for the IOC (e.g., the malware component name, C2 domain).\"},\n",
        "                        'description': {'type': 'string', 'description': \"Contextual notes about the IOC's purpose or origin.\"},\n",
        "                        'filename': {'type': 'string', 'description': \"REQUIRED only if type is 'file-path'. The name of the file.\"},\n",
        "                        'directory_path': {'type': 'string', 'description': \"REQUIRED only if type is 'file-path'. The path to the directory containing the file, potentially including variables like %LOCALAPPDATA%.\"}\n",
        "                    },\n",
        "                    'required': ['value', 'type', 'name', 'description']\n",
        "                }\n",
        "            }\n",
        "        },\n",
        "        'required': ['iocs']\n",
        "    }\n",
        "}\n",
        "\n",
        "# STAGE 1: REGEX-BASED TRIAGE\n",
        "def stage1_regex_triage(text: str) -> Dict[str, str]:\n",
        "    print(\"--- Stage 1: Starting regex triage ---\")\n",
        "    candidate_dict: Dict[str, str] = {}\n",
        "    try:\n",
        "        # Extract IPs\n",
        "        for ip in iocextract.extract_ips(text, refang=True):\n",
        "             # Check to avoid common local IPs if not desired\n",
        "             if ip not in ['127.0.0.1']:\n",
        "                candidate_dict[ip] = 'ipv4'\n",
        "\n",
        "        # Extract hashes\n",
        "        for h in iocextract.extract_hashes(text):\n",
        "            h_lower = h.lower()\n",
        "            if len(h_lower) == 32: candidate_dict[h_lower] = 'md5'\n",
        "            elif len(h_lower) == 40: candidate_dict[h_lower] = 'sha1'\n",
        "            elif len(h_lower) == 64: candidate_dict[h_lower] = 'sha256'\n",
        "\n",
        "        # Regex for URLs\n",
        "        for url in iocextract.extract_urls(text, refang=True):\n",
        "             candidate_dict[url] = 'url'\n",
        "\n",
        "        # Regular expression for specific directories and paths\n",
        "        path_re = re.compile(r'(/[\\w\\./\\-\\\\]+|%[a-zA-Z]+%[\\w\\\\\\./\\-]+)') # Generic regex for paths\n",
        "        for path in set(path_re.findall(text)):\n",
        "             if path not in candidate_dict:\n",
        "                 # Basic classification based on the presence of file extensions\n",
        "                 if '.' in path.split('/')[-1].split('\\\\')[-1] and not path.endswith(('/', '\\\\')):\n",
        "                     candidate_dict[path] = 'file-path' # Potentially a file path\n",
        "                 else:\n",
        "                     candidate_dict[path] = 'directory' # Probably a directory\n",
        "\n",
        "        print(f\"✅ Stage 1: Found {len(candidate_dict)} unique candidates via regex.\")\n",
        "        return candidate_dict\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Stage 1: Error during regex triage: {e}\")\n",
        "        return {}\n",
        "\n",
        "# STAGE 2: LLM-BASED CONTEXTUAL ANALYSIS\n",
        "def stage2_llm_analysis(text: str, openai_client: AzureOpenAI, deployment_name: str) -> List[Dict]:\n",
        "    print(\"\\\\n--- Stage 2: Starting deep contextual analysis with LLM ---\")\n",
        "    if not openai_client: return []\n",
        "\n",
        "    prompt = f\"\"\"As a senior CTI analyst, your task is to meticulously analyze the following threat report. Your goal is to identify ALL Indicators of Compromise (IOCs). Scrutinize the text to confirm that IOCs are presented in a malicious context.\n",
        "\n",
        "    Extract ALL genuine IOCs and structure them using the provided function schema. Pay close attention to the required 'type' for each IOC:\n",
        "    - Use 'ipv4' for IP addresses.\n",
        "    - Use 'domain-name' for domain names (e.g., example.com).\n",
        "    - Use 'url' for full URLs (e.g., http://example.com/path).\n",
        "    - Use 'md5', 'sha1', 'sha256' for file hashes. Ensure you associate hashes with the correct filename in the 'name' field.\n",
        "    - Use 'mutex' for mutex names.\n",
        "    - Use 'windows-registry-key' for registry keys.\n",
        "    - Use 'directory' for simple directory paths (e.g., /data2/.ztls/, /tmp/, C:\\\\Users\\\\Public).\n",
        "    - **Special Case: File Paths:** If an IOC represents a specific file within a directory (like '%LOCALAPPDATA%\\\\KeyStore\\\\KeyProv.dll' or '/data2/tmp/%s.ini'), use the type 'file-path'. For this type ONLY, you MUST ALSO provide the 'filename' (e.g., 'KeyProv.dll', '%s.ini') and the 'directory_path' (e.g., '%LOCALAPPDATA%\\\\KeyStore', '/data2/tmp') as separate fields in the output object. The 'value' field should contain the full path string.\n",
        "\n",
        "    For each IOC, extract its 'value', 'type', 'name', and 'description'. Only include indicators clearly associated with malicious activity described in the report.\n",
        "\n",
        "    ---REPORT TEXT:\n",
        "    {text}\n",
        "    ---\"\"\"\n",
        "\n",
        "    try:\n",
        "        response = openai_client.chat.completions.create(\n",
        "            model=deployment_name,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            functions=[IOC_FUNCTION_SCHEMA],\n",
        "            function_call={\"name\": \"extract_and_validate_iocs\"},\n",
        "            temperature=TEMP_IOC_EXTRACTION,\n",
        "            reasoning_effort=REASONING_IOC_EXTRACTION\n",
        "        )\n",
        "\n",
        "        result = response.choices[0].message\n",
        "        if result.function_call:\n",
        "            function_args = json.loads(result.function_call.arguments)\n",
        "            llm_iocs = function_args.get(\"iocs\", [])\n",
        "            print(f\"✅ Stage 2: LLM extracted and validated {len(llm_iocs)} IOCs.\")\n",
        "            return llm_iocs\n",
        "        else:\n",
        "             print(\"⚠️ Stage 2: LLM did not call the function. No IOCs extracted by LLM.\")\n",
        "             return []\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Stage 2: Error during LLM analysis: {e}\")\n",
        "        return []\n",
        "\n",
        "# STAGE 3: CONSOLIDATION AND SCORING\n",
        "def stage3_consolidate_and_score(regex_iocs: Dict[str, str], llm_iocs: List[Dict]) -> List[Dict]:\n",
        "    print(\"\\\\n--- Stage 3: Consolidating and scoring IOCs ---\")\n",
        "\n",
        "    final_iocs: Dict[str, Dict] = {}\n",
        "\n",
        "    # First process the IOCs from the LLM (most reliable by type and context)\n",
        "    for ioc in llm_iocs:\n",
        "        ioc_type = ioc.get(\"type\")\n",
        "        value = ioc.get(\"value\", \"\")\n",
        "        # Normalize hash cases\n",
        "        if ioc_type in ['md5', 'sha1', 'sha256']:\n",
        "            value = value.lower()\n",
        "\n",
        "        # Prepare the base object\n",
        "        final_ioc_data = {\n",
        "            \"value\": value,\n",
        "            \"type\": ioc_type,\n",
        "            \"name\": ioc.get('name', 'N/A'),\n",
        "            \"description\": ioc.get('description', ''),\n",
        "            \"confidence\": 'medium'\n",
        "        }\n",
        "        # Add specific fields for file paths if present and valid\n",
        "        if ioc_type == 'file-path':\n",
        "             if ioc.get(\"filename\") and ioc.get(\"directory_path\"):\n",
        "                 final_ioc_data[\"filename\"] = ioc.get(\"filename\")\n",
        "                 final_ioc_data[\"directory_path\"] = ioc.get(\"directory_path\")\n",
        "             else:\n",
        "                 print(f\"⚠️ LLM extracted file-path without required filename/directory_path: {value}\")\n",
        "                 continue\n",
        "\n",
        "        final_iocs[value] = final_ioc_data\n",
        "\n",
        "    # Compare with regex candidates to increase confidence\n",
        "    for regex_value, regex_type in regex_iocs.items():\n",
        "         # Normalise hash cases\n",
        "         if regex_type in ['md5', 'sha1', 'sha256']:\n",
        "             regex_value = regex_value.lower()\n",
        "         if regex_type == 'ipv4-addr': regex_type = 'ipv4'\n",
        "\n",
        "         if regex_value in final_iocs:\n",
        "             # If the LLM found the same value, it increases confidence.\n",
        "             final_iocs[regex_value]['confidence'] = 'high'\n",
        "\n",
        "    consolidated_list = list(final_iocs.values())\n",
        "    print(f\"✅ Stage 3: Consolidated to {len(consolidated_list)} total IOCs before filtering.\")\n",
        "    return consolidated_list\n",
        "\n",
        "# ORCHESTRATION AND EXECUTION\n",
        "def run_ioc_extraction_pipeline(report_text: str):\n",
        "    print(\"\\\\n=== Starting Advanced IOC Extraction Pipeline ====\")\n",
        "    if not isinstance(report_text, str) or not report_text.strip(): return None\n",
        "\n",
        "    regex_candidates = stage1_regex_triage(report_text)\n",
        "    llm_results = stage2_llm_analysis(report_text, client, DEPLOYMENT_NAME)\n",
        "    consolidated_iocs = stage3_consolidate_and_score(regex_candidates, llm_results)\n",
        "\n",
        "    # Filter by confidence\n",
        "    final_filtered_iocs = [ioc for ioc in consolidated_iocs if ioc.get('confidence') in ['high', 'medium']]\n",
        "    print(f\"\\\\n=== ✅ Pipeline Complete: Extracted {len(final_filtered_iocs)} High/Medium Confidence IOCs ===\")\n",
        "    print(json.dumps(final_filtered_iocs, indent=2))\n",
        "    return final_filtered_iocs\n",
        "\n",
        "# Run the pipeline\n",
        "extracted_iocs = []\n",
        "if text and client:\n",
        "    extracted_iocs = run_ioc_extraction_pipeline(text)\n",
        "else:\n",
        "    print(\"\\\\nSkipping IOC extraction: Ensure the 'text' variable contains the report and the OpenAI client is configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "69Sk5wVgfS9c"
      },
      "source": [
        "## **Part 4**: Programmatic SCO & Indicator Generation\n",
        "\n",
        "This block transforms the raw list of IOCs from the previous stage into formal STIX objects. Its primary goal is to create the foundational \"what to look for\" components of the threat intelligence.\n",
        "\n",
        "This task is performed in three steps:\n",
        "\n",
        "* **Step 1** (**SCO Creation**): Creates STIX Cyber Observable Objects (SCOs) for tangible entities like IP addresses, URLs, and mutexes.\n",
        "* **Step 2** (**Granular Indicator Creation**): Generates a separate `Indicator` object for each individual hash (MD5, SHA1, SHA256) to allow for detailed relationship mapping.\n",
        "* **Step** 3 (**Relationship Generation**): Automatically creates crucial relationships **`derived-from`**, links all `Indicator` objects that originate from the same piece of evidence (e.g., all hash indicators for the same file)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P17V6Bt2Dq-u"
      },
      "outputs": [],
      "source": [
        "from stix2 import (\n",
        "    File, Directory, IPv4Address, URL, Mutex, Indicator, Relationship, DomainName, WindowsRegistryKey # Aggiunti\n",
        ")\n",
        "import datetime as dt\n",
        "import re\n",
        "from typing import List, Dict, Tuple\n",
        "from itertools import combinations\n",
        "\n",
        "def stix_safe_string(s: str) -> str:\n",
        "    # Escape backslashes before single quotation marks\n",
        "    if isinstance(s, str):\n",
        "        # First: replace existing backslashes with double backslashes\n",
        "        # Second: replace single quotation marks with escaped single quotation marks.\n",
        "        return s.replace(\"\\\\\", \"\\\\\\\\\").replace(\"'\", \"\\\\'\")\n",
        "    return str(s)\n",
        "\n",
        "def generate_stix_pattern_single_hash(ioc_type: str, ioc_value: str) -> str:\n",
        "    return f\"[file:hashes.'{ioc_type.upper()}' = '{stix_safe_string(ioc_value)}']\"\n",
        "\n",
        "def generate_sco_and_indicators_with_relations(ioc_list: List[Dict]) -> Tuple[List, List, List]:\n",
        "    \"\"\"\n",
        "    Creates STIX SCOs, Indicator SDOs and 'derived-from' relationships.\n",
        "    Handles IPv4, DomainName, URL, Mutex, Directory, compound Paths (file+dir), Registry Keys, and Hashes.\n",
        "    \"\"\"\n",
        "    print(\"\\\\n=== Starting SCO & Indicator Generation with Derived-From Relationships ====\")\n",
        "\n",
        "    stix_scos = []\n",
        "    stix_indicators = []\n",
        "    stix_relationships = []\n",
        "    file_indicator_map = {}\n",
        "\n",
        "    # Step 1: Create SCOs and Indicators for non-hash types\n",
        "    for ioc in ioc_list:\n",
        "        ioc_type = ioc.get(\"type\")\n",
        "        ioc_value = ioc.get(\"value\", \"\")\n",
        "        ioc_name = ioc.get(\"name\")\n",
        "        ioc_desc = ioc.get(\"description\")\n",
        "\n",
        "        sco = None\n",
        "        indicator_pattern = None\n",
        "\n",
        "        try:\n",
        "            # Clean and validate IP\n",
        "            if ioc_type == \"ipv4\":\n",
        "                ip_cleaned = ioc_value.replace('[.]', '.').replace('[:]', ':')\n",
        "                ip_only = ip_cleaned.split(':')[0]\n",
        "                sco = IPv4Address(value=ip_only)\n",
        "                indicator_pattern = f\"[ipv4-addr:value = '{sco.value}']\"\n",
        "\n",
        "            # Domain Name Management\n",
        "            elif ioc_type == \"domain-name\":\n",
        "                domain_cleaned = ioc_value.replace('[.]', '.')\n",
        "                sco = DomainName(value=domain_cleaned)\n",
        "                indicator_pattern = f\"[domain-name:value = '{sco.value}']\"\n",
        "\n",
        "            elif ioc_type == \"url\":\n",
        "                url_cleaned = ioc_value.replace('[.]', '.').replace('[:]', ':')\n",
        "                url_value = url_cleaned\n",
        "                if not url_value.startswith(('http://', 'https://', 'ftp://')):\n",
        "                    print(f\"⚠️ URL IOC '{url_value}' lacks a scheme, add 'http://' by default.\")\n",
        "                    url_value = 'http://' + url_value\n",
        "                sco = URL(value=url_value)\n",
        "                indicator_pattern = f\"[url:value = '{stix_safe_string(sco.value)}']\"\n",
        "\n",
        "            elif ioc_type == \"mutex\":\n",
        "                sco = Mutex(name=ioc_value)\n",
        "                indicator_pattern = f\"[mutex:name = '{stix_safe_string(sco.name)}']\"\n",
        "\n",
        "            # Directory Management\n",
        "            elif ioc_type == \"directory\":\n",
        "                sco = Directory(path=ioc_value)\n",
        "                indicator_pattern = f\"[directory:path = '{stix_safe_string(sco.path)}']\"\n",
        "\n",
        "            # Composite Path Management\n",
        "            elif ioc_type == \"file-path\":\n",
        "                 filename = ioc.get(\"filename\")\n",
        "                 dir_path = ioc.get(\"directory_path\")\n",
        "                 if filename and dir_path:\n",
        "                     processed_path = dir_path\n",
        "                     stix_path = stix_safe_string(processed_path)\n",
        "                     stix_filename = stix_safe_string(filename)\n",
        "\n",
        "                     indicator_pattern = f\"[file:name = '{stix_filename}' AND file:parent_directory_ref.path LIKE '{stix_path}']\"\n",
        "                 else:\n",
        "                     print(f\"⚠️ Skipping file-path IOC due to missing filename/directory_path (in generate func): {ioc}\")\n",
        "                     continue\n",
        "\n",
        "            # Registry Key Management\n",
        "            elif ioc_type == \"windows-registry-key\":\n",
        "                sco = WindowsRegistryKey(key=ioc_value)\n",
        "                indicator_pattern = f\"[windows-registry-key:key = '{stix_safe_string(sco.key)}']\"\n",
        "\n",
        "            elif ioc_type in [\"md5\", \"sha1\", \"sha256\"]:\n",
        "                 continue\n",
        "\n",
        "            else:\n",
        "                 print(f\"ℹ️ Skipping IOC with unhandled type in generation: {ioc_type} - Value: {ioc_value}\")\n",
        "                 continue\n",
        "\n",
        "            # Indicator Creation\n",
        "            if indicator_pattern:\n",
        "                if sco:\n",
        "                    stix_scos.append(sco)\n",
        "\n",
        "                indicator = Indicator(\n",
        "                    allow_custom=True,\n",
        "                    name=ioc_name,\n",
        "                    description=ioc_desc,\n",
        "                    pattern_type=\"stix\",\n",
        "                    pattern=indicator_pattern,\n",
        "                    valid_from=dt.datetime.now(dt.timezone.utc)\n",
        "                )\n",
        "                stix_indicators.append(indicator)\n",
        "\n",
        "            elif ioc_type not in [\"md5\", \"sha1\", \"sha256\"]:\n",
        "                 print(f\"⚠️ Internal Logic Error: Pattern not generated for IOC: {ioc}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error processing IOC during STIX object creation: {ioc} - Error: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Step 2: Create an Indicator for EACH hash file\n",
        "    for ioc in ioc_list:\n",
        "        ioc_type = ioc.get(\"type\")\n",
        "        if ioc_type in [\"md5\", \"sha1\", \"sha256\"]:\n",
        "            ioc_value = ioc.get(\"value\", \"\").lower()\n",
        "            file_name_desc = ioc.get(\"name\")\n",
        "\n",
        "            if not ioc_value or not file_name_desc:\n",
        "                 print(f\"⚠️ Skipping HASH IOC due to missing value/name: {ioc}\")\n",
        "                 continue\n",
        "\n",
        "            try:\n",
        "                indicator = Indicator(\n",
        "                    allow_custom=True,\n",
        "                    name=f\"Indicator for {file_name_desc} ({ioc_type.upper()})\",\n",
        "                    description=ioc.get(\"description\"),\n",
        "                    pattern_type=\"stix\",\n",
        "                    pattern=generate_stix_pattern_single_hash(ioc_type, ioc_value),\n",
        "                    valid_from=dt.datetime.now(dt.timezone.utc)\n",
        "                )\n",
        "                stix_indicators.append(indicator)\n",
        "\n",
        "                if file_name_desc not in file_indicator_map:\n",
        "                    file_indicator_map[file_name_desc] = []\n",
        "                file_indicator_map[file_name_desc].append(indicator)\n",
        "            except Exception as e:\n",
        "                 print(f\"❌ Error processing HASH IOC during STIX Indicator creation: {ioc} - Error: {e}\")\n",
        "                 continue\n",
        "\n",
        "    # Step 3: Create 'derived-from' relationships\n",
        "    for file_name, indicators in file_indicator_map.items():\n",
        "        if len(indicators) > 1:\n",
        "            for ind1, ind2 in combinations(indicators, 2):\n",
        "                 try:\n",
        "                     stix_relationships.append(Relationship(ind1.id, 'derived-from', ind2.id))\n",
        "                 except Exception as e:\n",
        "                     print(f\"❌ Error creating derived-from relationship between {ind1.id} and {ind2.id} - Error: {e}\")\n",
        "\n",
        "    print(f\"✅ Generated {len(stix_scos)} SCOs, {len(stix_indicators)} Indicators, and {len(stix_relationships)} Relationships.\")\n",
        "    return stix_scos, stix_indicators, stix_relationships\n",
        "\n",
        "# Execution\n",
        "stix_scos, stix_indicators, stix_relationships = [], [], []\n",
        "if 'extracted_iocs' in locals() and extracted_iocs:\n",
        "    stix_scos, stix_indicators, stix_relationships = generate_sco_and_indicators_with_relations(extracted_iocs)\n",
        "    print(\"\\\\n--- Preview Generated Objects ---\")\n",
        "    print(f\"SCOs created: {len(stix_scos)}\")\n",
        "    print(f\"Indicators created: {len(stix_indicators)}\")\n",
        "    print(f\"Relationships created: {len(stix_relationships)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8r1kSHgu-M4"
      },
      "source": [
        "## **Part 5**: Comprehensive Entity Extraction (SDOs)\n",
        "\n",
        "With the low-level indicators defined, this block uses the LLM to understand the high-level context of the threat. It parses the entire CTI report to extract the core STIX Domain Objects (SDOs).\n",
        "\n",
        "The block consists of three main parts:\n",
        "\n",
        "* **LLM Prompting**: A detailed prompt instructs the model to act as a CTI analyst and identify `Malware`, `Attack Pattern`, `Vulnerability`, `Threat-Actor`, and `Identity` objects.\n",
        "* **Structured Extraction**: The model is tasked with extracting not only names and descriptions but also specific metadata like `malware_types` and `kill_chain_phases` from the MITRE ATT&CK table.\n",
        "* **JSON Output**: The result is a clean, structured list of entities that will form the narrative backbone of the final STIX bundle."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ngg4t68M04ZC"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def extract_all_entities_revised(report_text: str):\n",
        "    \"\"\"\n",
        "    Use an LLM to robustly extract all SDO entities,\n",
        "    including Attack Patterns from the MITRE table.\n",
        "    \"\"\"\n",
        "    print(\"\\n=== Starting Comprehensive Entity Extraction ===\")\n",
        "\n",
        "    if not client:\n",
        "        print(\"❌ OpenAI client is not configured. Aborting extraction.\")\n",
        "        return []\n",
        "\n",
        "    # Improved prompt that instructs the LLM to extract ALL entities, including Attack Patterns, from their specific table.\n",
        "    prompt = f\"\"\"\n",
        "    As a senior CTI analyst, your task is to meticulously identify and classify all distinct entities within the provided threat report that correspond to STIX Domain Object types.\n",
        "\n",
        "    Fundamental Rule: You must extract entities found EXCLUSIVELY within the report text provided after “--- THREAT REPORT TEXT ---”. The examples provided in these instructions serve ONLY as a guide to the format and MUST NOT be extracted.\n",
        "\n",
        "    Focus on the following STIX types: Malware, Attack-Pattern, Identity, Tool, Threat-Actor, and Vulnerability.\n",
        "\n",
        "    Instructions:\n",
        "    1.  Read the entire report to understand the context.\n",
        "    2.  For the **Malware** object, you MUST extract:\n",
        "        - Its `name`.\n",
        "        - Its `type` as \"malware\".\n",
        "        - A detailed `description`.\n",
        "        - A `malware_types` array, inferring the type from this list: [\"remote-access-trojan\", \"backdoor\", \"downloader\", \"spyware\", \"ransomware\"].\n",
        "    3.  For any **Identity** or **Tool** objects mentioned (e.g., NCSC, Trend Micro, PwC, VMware), extract their `name`, `type`, and a concise `description` of their role in the report.\n",
        "    4.  **Author Identification**: Identify the primary organization that authored or published this report (e.g., Cisco Talos, Mandiant, NCSC) and extract it as an 'identity' object.\n",
        "    5.  For any **Threat-Actor** (e.g., APT groups, specific threat actors), you MUST extract:\n",
        "        - Its `name` and any known `aliases`.\n",
        "        - Its `type` as \"threat-actor\".\n",
        "        - A detailed `description` of its goals, motivations, or relevant TTPs mentioned in the report.\n",
        "    6.  For any **Vulnerability** (e.g., CVEs), you MUST extract:\n",
        "        - Its `name` (the CVE identifier, e.g., \"CVE-2021-44228\").\n",
        "        - Its `type` as \"vulnerability\".\n",
        "        - A `description` of how the vulnerability is exploited according to the report.\n",
        "    7.  **YARA Rule Extraction**: Search for any YARA rule blocks. For EACH rule, you MUST extract:\n",
        "        - The `name` of the rule.\n",
        "        - The `type` as \"yara-rule\".\n",
        "        - The `pattern`, which is the ENTIRE text of the rule.\n",
        "        - An `indicates_malware` field containing the lowercase name of the primary malware this rule detects (e.g., \"umbrella stand\").\n",
        "        - An `associated_hashes` array containing a list of any file hashes (MD5, SHA1, or SHA256) that the text directly associates with this rule.\n",
        "    8.  Specifically locate the **'MITRE ATT&CK®' table**. For EACH row in that table, you MUST extract the Attack Pattern.\n",
        "    9.  For each **Attack-Pattern**, you MUST extract:\n",
        "        - The `name` (from the \"Technique\" column).\n",
        "        - The `type` as \"attack-pattern\".\n",
        "        - The `description` (from the \"Procedure\" column).\n",
        "        - The `external_id` (from the \"ID\" column, e.g., \"T1129\").\n",
        "        - The `kill_chain_phases` as an array with a single object containing the phase name (from the \"Tactic\" column, e.g., {{\"kill_chain_name\": \"mitre-attack\", \"phase_name\": \"execution\"}}).\n",
        "    10.  **Primary Subject Identification**: After analyzing the report, identify the primary malware family that is the main topic and place its lowercase name in a root-level JSON key called \"primary_malware_subject\".\n",
        "    11.  Format the entire output as a single, valid JSON object with TWO root keys: \"primary_malware_subject\" and \"entities\". The value of \"entities\" must be an array of the extracted objects.\n",
        "\n",
        "    Example of a final object in the array (FOR FORMATTING REFERENCE ONLY):\n",
        "    {{\n",
        "      \"name\": \"Name-Technique-Example\",\n",
        "      \"type\": \"attack-pattern\",\n",
        "      \"description\": \"Description of how the sample malware uses this technique...\",\n",
        "      \"external_id\": \"TXXXX\",\n",
        "      \"kill_chain_phases\": [\n",
        "        {{\n",
        "          \"kill_chain_name\": \"mitre-attack\",\n",
        "          \"phase_name\": \"tactic-name-example\"\n",
        "        }}\n",
        "      ]\n",
        "    }}\n",
        "    Formatting example for YARA rule\n",
        "    {{\n",
        "      \"name\": \"YARA_RULE_EXAMPLE_NAME\",\n",
        "      \"type\": \"yara-rule\",\n",
        "      \"pattern\": \"rule YARA_RULE_EXAMPLE_NAME {{ meta: ... strings: ... condition: ... }}\",\n",
        "      \"indicates_malware\": \"nome-malware-esempio\",\n",
        "      \"associated_hashes\": [\"hash_sha256_del_file_di_esempio\"]\n",
        "    }}\n",
        "\n",
        "    --- THREAT REPORT TEXT ---\n",
        "    {report_text}\n",
        "    --- END OF REPORT ---\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        print(\"▶️ Sending request to Azure OpenAI API for comprehensive entity extraction...\")\n",
        "        response = client.chat.completions.create(\n",
        "            model=DEPLOYMENT_NAME,\n",
        "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
        "            temperature=TEMP_SDO_EXTRACTION,\n",
        "            response_format={\"type\": \"json_object\"},\n",
        "            reasoning_effort=REASONING_SDO_EXTRACTION\n",
        "        )\n",
        "\n",
        "        raw_response_content = response.choices[0].message.content\n",
        "\n",
        "        # Debug: Print the raw response for inspection\n",
        "        print(\"\\n--- DEBUG: Raw LLM Response ---\")\n",
        "        print(raw_response_content)\n",
        "        print(\"-----------------------------\\n\")\n",
        "\n",
        "        result_json = json.loads(raw_response_content)\n",
        "\n",
        "        # Extracts the list of entities from the root key 'entities'\n",
        "        entity_list = result_json.get(\"entities\", [])\n",
        "        if not entity_list:\n",
        "             print(\"⚠️ Warning: The LLM returned a valid JSON but the 'entities' list is empty.\")\n",
        "\n",
        "        print(f\"✅ LLM Extraction complete. Total entities identified: {len(entity_list)}.\")\n",
        "        return result_json\n",
        "\n",
        "    except json.JSONDecodeError as e:\n",
        "        print(f\"❌ CRITICAL ERROR: Failed to decode JSON from the LLM response. Error: {e}\")\n",
        "        print(\"   Check the raw LLM response above to diagnose the issue.\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"❌ CRITICAL ERROR during SDO entity extraction: {e}\")\n",
        "        return []\n",
        "\n",
        "# --- Extraction execution ---\n",
        "sdo_entities = []\n",
        "# new variable for the name of the main malware\n",
        "main_malware_name = None\n",
        "if text and client:\n",
        "    extraction_result = extract_all_entities_revised(text)\n",
        "    if extraction_result:\n",
        "        sdo_entities = extraction_result.get(\"entities\", [])\n",
        "        # extracts the output name from the LLM\n",
        "        main_malware_name = extraction_result.get(\"primary_malware_subject\")\n",
        "        print(f\"\\\\n✅ Main subject identified by LLM: {main_malware_name}\")\n",
        "    print(\"\\\\n--- Final Extracted SDO Entities ---\")\n",
        "    print(json.dumps(sdo_entities, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HWvBRLUo6Azv"
      },
      "source": [
        "## **Part 6**: Final Assembly and Bundling\n",
        "\n",
        "This is the final stage where all previously generated STIX objects are brought together to create a single, cohesive, and interoperable intelligence package.\n",
        "\n",
        "* **Object Aggregation**: Gathers all SDOs, SCOs, and SROs created in the previous blocks into a single list.\n",
        "* **Contextual Relationship Creation**: Creates the high-level relationships that connect the threat narrative, such as linking the `Malware` object to the `Attack Patterns` it `uses` and the `Indicators` that `indicate` its presence.\n",
        "* **Report Object Generation**: Creates a top-level `Report` object that summarizes the analysis and references all other objects in the bundle.\n",
        "* **Bundle Creation & Serialization**: Assembles all objects into a final STIX 2.1 `Bundle` and saves it as a timestamped JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oK6iQrNb_AgJ"
      },
      "outputs": [],
      "source": [
        "# STIX2 Imports\n",
        "from stix2 import (Identity, Malware, AttackPattern, Relationship, Bundle, Report, MarkingDefinition, TLP_WHITE)\n",
        "import datetime as dt\n",
        "import os\n",
        "\n",
        "# Variable to contain the final bundle\n",
        "final_bundle = None\n",
        "# List to contain all STIX objects before bundling\n",
        "all_stix_objects = []\n",
        "\n",
        "# Check that the variables required by the previous blocks exist\n",
        "if 'sdo_entities' in locals() and 'stix_scos' in locals() and 'stix_indicators' in locals() and 'stix_relationships' in locals():\n",
        "    print(\"▶️ Start assembly of final STIX bundle...\")\n",
        "\n",
        "    # --- 1. Create fundamental metadata objects (with smarter dynamic author) ---\n",
        "    identity_author = None\n",
        "    author_keywords = ['author', 'published', 'researchers', 'report', 'responsible for this analysis'] # Parole chiave per identificare l'autore\n",
        "\n",
        "    # Search for a specific author among the extracted entities, based on keywords.\n",
        "    for entity in sdo_entities:\n",
        "        if entity.get(\"type\") == \"identity\":\n",
        "            # Check if the description contains any of our keywords.e\n",
        "            description = entity.get(\"description\", \"\").lower()\n",
        "            if any(keyword in description for keyword in author_keywords):\n",
        "                print(f\"✅ Author identified by keywords: '{entity.get('name')}'\")\n",
        "                identity_author = Identity(\n",
        "                    name=entity.get(\"name\"),\n",
        "                    identity_class=\"organization\",\n",
        "                    description=entity.get(\"description\")\n",
        "                )\n",
        "                all_stix_objects.append(identity_author)\n",
        "                break\n",
        "\n",
        "    # If no specific author was found after the cycle, use a default.\n",
        "    if not identity_author:\n",
        "        print(\"⚠️ No specific author found, ‘NCSC’ is used as default.\")\n",
        "        identity_author = Identity(name=\"NCSC\", identity_class=\"organization\")\n",
        "        all_stix_objects.append(identity_author)\n",
        "\n",
        "    # Add other metadata\n",
        "    tlp_clear = TLP_WHITE\n",
        "    all_stix_objects.append(tlp_clear)\n",
        "    print(f\"✅ Author's Identity Object aimed at: '{identity_author.name}'\")\n",
        "\n",
        "    # --- 2. Add the SCOs, Indicators, and their relationships from the previous blocks ---\n",
        "    all_stix_objects.extend(stix_scos)\n",
        "    all_stix_objects.extend(stix_indicators)\n",
        "    all_stix_objects.extend(stix_relationships)\n",
        "    print(f\"✅ Added {len(stix_scos)} SCOs, {len(stix_indicators)} Indicators, and {len(stix_relationships)} 'based-on' relationships.\")\n",
        "\n",
        "    # --- 3. Create SDOs (Malware, Attack Patterns, etc.) from the ‘sdo_entities’ list ---\n",
        "    created_sdos = {}\n",
        "    malware_main_obj = None\n",
        "\n",
        "    for entity in sdo_entities:\n",
        "        entity_type = entity.get(\"type\")\n",
        "        entity_name = entity.get(\"name\")\n",
        "        sdo = None\n",
        "\n",
        "        if entity_type == \"malware\":\n",
        "            sdo = Malware(\n",
        "                name=entity_name.lower(),\n",
        "                is_family=True,\n",
        "                description=entity.get(\"description\"),\n",
        "                malware_types=entity.get(\"malware_types\", [\"remote-access-trojan\"]),\n",
        "                created_by_ref=identity_author.id\n",
        "            )\n",
        "            created_sdos[entity_name] = sdo\n",
        "\n",
        "        elif entity_type == \"attack-pattern\":\n",
        "            # NEW LOGIC: Correctly formats kill_chain_phases\n",
        "            kill_chain_phases = entity.get(\"kill_chain_phases\", [])\n",
        "            for phase in kill_chain_phases:\n",
        "                if 'phase_name' in phase:\n",
        "                    phase['phase_name'] = phase['phase_name'].lower().replace(' ', '-')\n",
        "\n",
        "            sdo = AttackPattern(\n",
        "                name=entity_name,\n",
        "                description=entity.get(\"description\"),\n",
        "                created_by_ref=identity_author.id,\n",
        "                kill_chain_phases=kill_chain_phases, # Use the formatted list\n",
        "                external_references=[{\n",
        "                    \"source_name\": \"mitre-attack\",\n",
        "                    \"external_id\": entity.get(\"external_id\"),\n",
        "                    \"url\": f\"https://attack.mitre.org/techniques/{entity.get('external_id').replace('.', '/')}\"\n",
        "                }]\n",
        "            )\n",
        "            created_sdos[entity.get(\"external_id\")] = sdo\n",
        "\n",
        "        elif entity_type == \"threat-actor\":\n",
        "            sdo = ThreatActor(\n",
        "                name=entity.get(\"name\"),\n",
        "                description=entity.get(\"description\"),\n",
        "                aliases=entity.get(\"aliases\", []),\n",
        "                created_by_ref=identity_author.id\n",
        "            )\n",
        "            created_sdos[entity_name] = sdo\n",
        "\n",
        "        elif entity_type == \"vulnerability\":\n",
        "            sdo = Vulnerability(\n",
        "                name=entity.get(\"name\"),\n",
        "                description=entity.get(\"description\"),\n",
        "                created_by_ref=identity_author.id\n",
        "            )\n",
        "            created_sdos[entity_name] = sdo\n",
        "\n",
        "        elif entity_type == \"yara-rule\":\n",
        "             sdo = Indicator(\n",
        "                 name=entity.get(\"name\"),\n",
        "                 description=f\"YARA rule to detect related activity.\",\n",
        "                 pattern_type=\"yara\",\n",
        "                 pattern=entity.get(\"pattern\"),\n",
        "                 created_by_ref=identity_author.id,\n",
        "                 valid_from=dt.datetime.now(dt.timezone.utc)\n",
        "             )\n",
        "\n",
        "        elif entity_type == \"identity\":\n",
        "             # Check if this identity is the author we have ALREADY created, skip it to avoid creating a duplicate..\n",
        "             if entity.get(\"name\") == identity_author.name:\n",
        "                 continue\n",
        "\n",
        "             # Otherwise, if it is another identity (e.g., a victim), create it as usual..\n",
        "             sdo = Identity(\n",
        "                 name=entity.get(\"name\"),\n",
        "                 identity_class=\"organization\",\n",
        "                 description=entity.get(\"description\")\n",
        "             )\n",
        "\n",
        "        if sdo:\n",
        "            all_stix_objects.append(sdo)\n",
        "\n",
        "    # --- Find the main malware object dynamically ---\n",
        "    if main_malware_name:\n",
        "        malware_main_obj = created_sdos.get(main_malware_name)\n",
        "\n",
        "    # Fallback in case the name does not match or has not been found\n",
        "    if not malware_main_obj:\n",
        "        print(\"⚠️ Main malware subject not found via LLM suggestion. Attempting to find first malware in list.\")\n",
        "        # Search for the first malware object created as a last resort\n",
        "        for obj in all_stix_objects:\n",
        "            if obj.type == 'malware':\n",
        "                malware_main_obj = obj\n",
        "                break\n",
        "\n",
        "    if malware_main_obj:\n",
        "        print(f\"✅ Main malware object for relationships set to: '{malware_main_obj.name}'\")\n",
        "\n",
        "    print(f\"✅ Created {len(created_sdos)} SDOs (Malware, Attack Patterns, Identities).\")\n",
        "\n",
        "    # --- 4. Create contextual relationships (SROs) ---\n",
        "    print(\"⏳ Creating contextual relationships...\")\n",
        "    if malware_main_obj:\n",
        "        for entity in sdo_entities:\n",
        "            if entity.get(\"type\") == \"attack-pattern\":\n",
        "                attack_pattern_obj = created_sdos.get(entity.get(\"external_id\"))\n",
        "                if attack_pattern_obj:\n",
        "                    rel = Relationship(malware_main_obj.id, 'uses', attack_pattern_obj.id, created_by_ref=identity_author.id)\n",
        "                    all_stix_objects.append(rel)\n",
        "\n",
        "        for indicator in stix_indicators:\n",
        "            rel = Relationship(indicator.id, 'indicates', malware_main_obj.id, created_by_ref=identity_author.id)\n",
        "            all_stix_objects.append(rel)\n",
        "\n",
        "        print(\"✅ 'Uses' and ‘indicates’ relationships successfully created.\")\n",
        "    else:\n",
        "        print(\"⚠️ Warning: Main malware object not found. Unable to create relationships..\")\n",
        "\n",
        "    # --- Accurate YARA reports ---\n",
        "    print(\"⏳ Creating accurate relationships for YARA indicators...\")\n",
        "\n",
        "    yara_entities = [e for e in sdo_entities if e.get('type') == 'yara-rule']\n",
        "    yara_indicators = [o for o in all_stix_objects if o.type == 'indicator' and o.pattern_type == 'yara']\n",
        "\n",
        "    new_yara_rels = 0\n",
        "    for entity in yara_entities:\n",
        "        # Find the corresponding YARA indicator object\n",
        "        yara_indicator = next((yi for yi in yara_indicators if yi.name == entity.get('name')), None)\n",
        "        if not yara_indicator:\n",
        "            continue\n",
        "\n",
        "        # 1. Create the precise “indicates” relationship\n",
        "        malware_name = entity.get('indicates_malware')\n",
        "        # Search for the corresponding malware among the objects already created\n",
        "        malware_obj = next((obj for obj in all_stix_objects if obj.type == 'malware' and obj.name == malware_name), None)\n",
        "        if malware_obj:\n",
        "            rel_indicates = Relationship(yara_indicator.id, 'indicates', malware_obj.id, created_by_ref=identity_author.id)\n",
        "            all_stix_objects.append(rel_indicates)\n",
        "            new_yara_rels += 1\n",
        "\n",
        "        # 2. Create accurate “derived-from” relationships\n",
        "        associated_hashes = entity.get('associated_hashes', [])\n",
        "        for h in associated_hashes:\n",
        "            # Find the corresponding hash indicator\n",
        "            hash_indicator = next((ind for ind in stix_indicators if h in ind.pattern), None)\n",
        "            if hash_indicator:\n",
        "                rel_derived = Relationship(yara_indicator.id, 'derived-from', hash_indicator.id, created_by_ref=identity_author.id)\n",
        "                all_stix_objects.append(rel_derived)\n",
        "                new_yara_rels += 1\n",
        "\n",
        "    if new_yara_rels > 0:\n",
        "        print(f\"✅ Create {new_yara_rels} New precise relationships for YARA indicators.\")\n",
        "    else:\n",
        "        print(\"ℹ️ No YARA reports to create based on LLM output.\")\n",
        "\n",
        "    # --- Add relationships for Threat Actors and Vulnerabilities ---\n",
        "    print(\"⏳ Creating Threat Actor and Vulnerability Reports...\")\n",
        "\n",
        "    # Find all the new items we have created\n",
        "    threat_actors = [obj for obj in all_stix_objects if obj.type == 'threat-actor']\n",
        "    vulnerabilities = [obj for obj in all_stix_objects if obj.type == 'vulnerability']\n",
        "\n",
        "    new_context_rels = 0\n",
        "    if threat_actors and malware_main_obj:\n",
        "        # Create the “uses” relationship: Threat Actor -> uses -> Malware\n",
        "        for ta in threat_actors:\n",
        "            rel = Relationship(ta.id, 'uses', malware_main_obj.id, created_by_ref=identity_author.id)\n",
        "            all_stix_objects.append(rel)\n",
        "            new_context_rels += 1\n",
        "\n",
        "    if vulnerabilities and malware_main_obj:\n",
        "        # Create the relationship “exploits”: Malware -> exploits -> Vulnerability\n",
        "        for vuln in vulnerabilities:\n",
        "            rel = Relationship(malware_main_obj.id, 'exploits', vuln.id, created_by_ref=identity_author.id)\n",
        "            all_stix_objects.append(rel)\n",
        "            new_context_rels += 1\n",
        "\n",
        "    if new_context_rels > 0:\n",
        "        print(f\"✅ Create {new_context_rels} new contextual relationships.\")\n",
        "    else:\n",
        "        print(\"ℹ️ No new reports for Threat Actor or Vulnerability to create.\")\n",
        "\n",
        "    # --- 5. Create the Report object to contextualize the bundle ---\n",
        "    valid_object_refs = [obj.id for obj in all_stix_objects if obj.type != 'marking-definition']\n",
        "    report_obj = Report(\n",
        "        name=f\"Analisi Malware: {malware_main_obj.name.title() if malware_main_obj else 'Threat Report'}\",\n",
        "        description=f\"This report contains technical analysis and indicators associated with malware. {malware_main_obj.name if malware_main_obj else 'unknown'}.\",\n",
        "        published=dt.datetime.now(dt.timezone.utc),\n",
        "        created_by_ref=identity_author.id,\n",
        "        object_marking_refs=[tlp_clear.id],\n",
        "        report_types=[\"threat-report\"],\n",
        "        object_refs=valid_object_refs\n",
        "    )\n",
        "    all_stix_objects.append(report_obj)\n",
        "    print(\"✅ Object Report created.\")\n",
        "\n",
        "    # --- 6. Create the final bundle ---\n",
        "    final_bundle = Bundle(*all_stix_objects)\n",
        "\n",
        "    print(\"\\n🎉 --- STIX 2.1 Bundle Generated Correctly --- 🎉\")\n",
        "    print(f\"Total objects in bundle: {len(final_bundle.objects)}\")\n",
        "\n",
        "    # --- 7. Save the bundle to a file with a timestamp ---\n",
        "    # Set the path for the output folder on Google Drive\n",
        "    stix_reports_folder = '/content/drive/MyDrive/Reports/STIX_Reports'\n",
        "\n",
        "    # Create the folder if it does not already exist\n",
        "    os.makedirs(stix_reports_folder, exist_ok=True)\n",
        "\n",
        "    # Generate a base name for the file, checking if a PDF name exists\n",
        "    if 'pdf_input_name' in locals() and pdf_input_name:\n",
        "      # If using PDF input, use its name\n",
        "      base_name = os.path.splitext(pdf_input_name)[0]\n",
        "    else:\n",
        "      # Otherwise, use a generic name\n",
        "      base_name = \"cti_report\"\n",
        "\n",
        "    # Generate the filename\n",
        "    timestamp = datetime.now().strftime('%Y_%m_%d_%H_%M')\n",
        "    stix_filename = f\"{base_name}_bundle_{timestamp}.json\"\n",
        "\n",
        "    # Combine the folder path and file name to obtain the full path\n",
        "    full_save_path = os.path.join(stix_reports_folder, stix_filename)\n",
        "\n",
        "    # Write the STIX bundle to the file\n",
        "    with open(full_save_path, 'w') as file:\n",
        "        file.write(final_bundle.serialize(pretty=True)) # Serialize the bundle to JSON string before writing\n",
        "\n",
        "    print(\"\\n--- Saving Complete ---\")\n",
        "    print(f\"STIX bundle successfully saved in: {full_save_path}\")\n",
        "    print(final_bundle.serialize(pretty=True))\n",
        "\n",
        "else:\n",
        "    print(\"❌ ERRORE: Variabili necessarie non trovate. Esegui i blocchi precedenti.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UfR9umuXXua"
      },
      "source": [
        "## **Part 7**: Populating GitHub repo\n",
        "This section deals with automatically populating a public GitHub repository with the generated STIX bundles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qZlbRN6jXrn3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "print(\"--- Start Synchronization with GitHub ---\")\n",
        "\n",
        "try:\n",
        "    # Retrieve credentials from Colab Secrets\n",
        "    token = userdata.get('GITHUB_PAT')\n",
        "    username = userdata.get('GITHUB_USERNAME')\n",
        "    repo_name = userdata.get('GITHUB_REPO_NAME')\n",
        "\n",
        "    # Check that all secrets have been set\n",
        "    if not all([token, username, repo_name]):\n",
        "        raise ValueError(\"Make sure you have set GITHUB_PAT, GITHUB_USERNAME, and GITHUB_REPO_NAME in Colab Secrets.\")\n",
        "\n",
        "    print(f\"GitHub credentials retrieved. Target repository: {repo_name}\")\n",
        "\n",
        "    # Clone the repository from GitHub using the authentication token\n",
        "    # Removes the folder if it already exists to ensure a clean state at each execution\n",
        "    !rm -rf {repo_name}\n",
        "\n",
        "    repo_url = f\"https://{token}@github.com/{username}/{repo_name}.git\"\n",
        "    !git clone {repo_url}\n",
        "\n",
        "    # Copy the generated JSON file (from Drive) to the local repository folder\n",
        "    if 'full_save_path' in locals() and os.path.exists(full_save_path):\n",
        "        !cp \"{full_save_path}\" \"{repo_name}/\"\n",
        "        stix_filename = os.path.basename(full_save_path)\n",
        "        print(f\"File '{stix_filename}' copied to the local repository.\")\n",
        "\n",
        "        # Run the Git commands to commit and push the new file.\n",
        "        %cd {repo_name}\n",
        "        !git config user.name \"{username}\"\n",
        "        !git config user.email \"{username}@users.noreply.github.com\"\n",
        "        !git add .\n",
        "\n",
        "        commit_message = f\"Add STIX bundle: {stix_filename}\"\n",
        "        !git commit -m \"{commit_message}\"\n",
        "\n",
        "        !git push\n",
        "\n",
        "        print(f\"\\nPush completed successfully\")\n",
        "        print(f\"You can view the file at: https://github.com/{username}/{repo_name}\")\n",
        "\n",
        "        # Return to the main Colab work directory\n",
        "        %cd /content\n",
        "    else:\n",
        "        print(\"ERROR: The variable ‘full_save_path’ with the STIX file path was not found or the file does not exist.\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"An error occurred while synchronizing with GitHub: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2AEP_Wk4Yibx"
      },
      "source": [
        "## **Part 8**: STIX Viewer\n",
        "STIX 2.1 bundle Visualizer.\n",
        "\n",
        "To visualize the STIX bundle, we use the cti-stix-visualization project, inserted as an iFrame in the notebook.\n",
        "\n",
        "[STIX Visualizer](https://oasis-open.github.io/cti-stix-visualization/)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BdRV4c9DYnad"
      },
      "outputs": [],
      "source": [
        "# Visualize STIX Bundle\n",
        "# cut & paste json file\n",
        "from IPython.display import IFrame\n",
        "\n",
        "IFrame(src='https://oasis-open.github.io/cti-stix-visualization/', width=1200, height=1000)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "IWRCXLbHw6IS",
        "pWNOvrqge0o4"
      ],
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}