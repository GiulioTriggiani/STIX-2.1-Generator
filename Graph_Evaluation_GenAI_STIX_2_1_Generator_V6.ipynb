{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Evaluation for GenAI-Powered STIX 2.1 Generator**\n",
        "\n",
        "**Notebook Version:** 6.0  \n",
        "**Author:** Giulio Triggiani  \n",
        "**Python Version:** >= 3.8  \n",
        "**Key Libraries:** `stix2`\n",
        "\n",
        "---\n",
        "\n",
        "## **Objective**\n",
        "The objective is to **quantitatively** evaluate the performance of GenAI_STIX_2_1_Generator, an LLM-based tool for the automatic generation of Cyber Threat Intelligence reports in STIX 2.1 format.\n",
        "\n",
        "To measure the effectiveness of the generator, this script compares a bundle automatically generated by the tool with a reference bundle (‚Äúground truth‚Äù) manually created by a CTI analyst.\n",
        "\n",
        "The analysis leverages the advanced features of the official stix2 library, performing in particular a comprehensive semantic comparison of the bundle graph to identify not only literal matches but also semantic ones.\n",
        "\n",
        "The data obtained from these comparisons are then used to calculate the overall averages of the bundles compared.\n",
        "\n",
        "## **Workflow Overview**\n",
        "\n",
        "1.   **Setup**: installs the Python libraries needed for validating and manipulating STIX objects;\n",
        "2.   **Libraries and Environment**: imports the required modules and mount Google Drive to access the file;\n",
        "3.   **Support Functions**: This cell contains the main functions that perform validation and display the data in tabular form;\n",
        "5.   **Comparison**: performs the entire process: it browses folders, loads bundles, compares them, and prints the results.\n"
      ],
      "metadata": {
        "id": "wSrXE79msx4M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 1**: Setup\n",
        "\n",
        "This block installs the Python libraries needed for validating and manipulating STIX objects."
      ],
      "metadata": {
        "id": "d7ZWDNcrEyDj"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zn1X6pO5DAuw"
      },
      "outputs": [],
      "source": [
        "print(\"--- Installing dependencies ---\")\n",
        "!pip install stix2[semantic] --quiet\n",
        "!pip install stix2-validator --quiet\n",
        "print(\"Installation complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 2**: Importing Libraries and Setting Up the Environment\n",
        "\n",
        "This block imports the required modules and mounts Google Drive to access the files.\n",
        "\n",
        "NOTE: The first time you will be asked for access to Google Drive."
      ],
      "metadata": {
        "id": "_mjmdirsG1cT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "from collections import defaultdict\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "# Import from the stix2 library\n",
        "from stix2 import parse, MemoryStore\n",
        "from stix2.equivalence.graph import graph_similarity\n",
        "from stix2 import Relationship\n",
        "from stix2validator import validate_file, print_results\n",
        "\n",
        "from google.colab import drive\n",
        "\n",
        "print(\"--- Google Drive Mount ---\")\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "print(\"Google Drive mounted correctly.\")\n",
        "\n",
        "# --- CONFIGURATION ---\n",
        "BASE_DRIVE_DIR = '/content/drive/MyDrive/Reports_Evaluation'\n",
        "OUTPUT_DIR = '/content/drive/MyDrive/Evaluation_Results'"
      ],
      "metadata": {
        "id": "d-tt_czFHOjF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 3**: Support Functions\n",
        "This cell contains the main functions that perform validation and display the data in tabular form.\n",
        "\n",
        "Bundles are validated according to the STIX 2.1 standard."
      ],
      "metadata": {
        "id": "UYWHHh5Ef9-Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def validate_bundle(file_path):\n",
        "    \"\"\" Validates an STIX 2.1 bundle. \"\"\"\n",
        "    print(f\"\\nValidating the bundle: {os.path.basename(file_path)}...\")\n",
        "    results = validate_file(file_path)\n",
        "    if results.is_valid:\n",
        "        print(f\"‚úÖ Validation of {os.path.basename(file_path)} success.\")\n",
        "        return True\n",
        "    else:\n",
        "        print(f\"‚ùå Validation ERROR for {os.path.basename(file_path)}:\")\n",
        "        print_results(results)\n",
        "        return False\n",
        "\n",
        "def display_scores_as_table(json_file_path):\n",
        "    \"\"\"\n",
        "    Reads a JSON file with prop_scores, processes it,\n",
        "    displays the results, and RETURNS the calculated statistics.\n",
        "    \"\"\"\n",
        "    # Set pandas to display all rows of the DataFrame and display the data in tabular form.\n",
        "    pd.set_option('display.max_rows', None)\n",
        "    try:\n",
        "        with open(json_file_path, 'r') as f: data = json.load(f)\n",
        "\n",
        "        summary = data.get('summary', {})\n",
        "        if not summary:\n",
        "            print(\"The JSON file does not contain the ‚Äòsummary‚Äô section or it is empty..\")\n",
        "            return None, None, None\n",
        "\n",
        "        # Extracts relevant information from the ‚Äòsummary‚Äô dictionary\n",
        "        results_list = [{\"Type Object\": k.split('--')[0], \"Object ID (Generated)\": v.get('lhs'),\n",
        "                         \"Object ID (Ground Truth)\": v.get('rhs'), \"Match Score (%)\": v.get('value', 0)}\n",
        "                        for k, v in summary.items()]\n",
        "\n",
        "        # Create a DataFrame with pandas\n",
        "        df = pd.DataFrame(results_list)\n",
        "\n",
        "        # Sort the DataFrame by score in descending order\n",
        "        df_sorted = df.sort_values(by=\"Match Score (%)\", ascending=False).reset_index(drop=True)\n",
        "\n",
        "        # Format the score to display it with two decimal places.\n",
        "        df_sorted[\"Match Score (%)\"] = df_sorted[\"Match Score (%)\"].map('{:,.2f}'.format)\n",
        "\n",
        "        print(\"\\n--- Detailed Analysis of Match Scores ---\")\n",
        "        # View the table in the notebook\n",
        "        display(df_sorted)\n",
        "\n",
        "        # Calculate the statistics to be returned\n",
        "        df['Match Score (%)'] = pd.to_numeric(df['Match Score (%)'])\n",
        "        summary_stats = df['Match Score (%)'].describe()\n",
        "        avg_score_by_type = df.groupby('Type Object')['Match Score (%)'].mean()\n",
        "\n",
        "        print(\"\\n--- Summary Statistics (Single Bundle) ---\")\n",
        "        display(summary_stats.to_frame().style.format('{:,.2f}'))\n",
        "        print(\"\\n--- Average Score by Item Type (Single Bundle) ---\")\n",
        "        display(avg_score_by_type.to_frame().sort_values(by=\"Match Score (%)\", ascending=False).style.format('{:,.2f}'))\n",
        "\n",
        "        # Returns the calculated data\n",
        "        return summary_stats, avg_score_by_type, df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR in display_scores_as_table: {e}\")\n",
        "        return None, None, None\n"
      ],
      "metadata": {
        "id": "NZWNJTJHgO8c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Part 4**: Main Logic of Comparison\n",
        "This is the main cell that performs the entire process: it browses folders, loads bundles, compares them, and prints the results.\n",
        "\n",
        "NOTE: All STIX Indicator objects with a `pattern-type` other than `stix` are excluded from the comparison.\n"
      ],
      "metadata": {
        "id": "1pR8CfxGQMc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_single_graph_score(generated_bundle_path, expert_bundle_path, full_output_path):\n",
        "    \"\"\"\n",
        "    Runs graph_similarity, captures the prop_scores, saves them to a JSON file and shows averages in tabular form.\n",
        "    \"\"\"\n",
        "    if not validate_bundle(generated_bundle_path) or not validate_bundle(expert_bundle_path):\n",
        "        return None, None, None, None\n",
        "\n",
        "    # Load bundles using json.load()\n",
        "    with open(generated_bundle_path, 'r') as f: gen_bundle_dict = json.load(f)\n",
        "    with open(expert_bundle_path, 'r') as f: exp_bundle_dict = json.load(f)\n",
        "\n",
        "    # Isolate and remove special markers to avoid the parser bug\n",
        "    gen_objects_to_compare = [obj for obj in gen_bundle_dict.get('objects', []) if not (obj.get('type') == 'indicator' and obj.get('pattern_type') != 'stix')]\n",
        "    exp_objects_to_compare = [obj for obj in exp_bundle_dict.get('objects', []) if not (obj.get('type') == 'indicator' and obj.get('pattern_type') != 'stix')]\n",
        "\n",
        "    ds_generated = MemoryStore(stix_data=gen_objects_to_compare)\n",
        "    ds_ground_truth = MemoryStore(stix_data=exp_objects_to_compare)\n",
        "\n",
        "    prop_scores = {}\n",
        "\n",
        "    print(\"\\n--- Performing graph-based comparison... ---\")\n",
        "    try:\n",
        "        score = graph_similarity(ds_generated, ds_ground_truth, prop_scores=prop_scores)\n",
        "\n",
        "        print(f\"‚úÖ Graph Similarity Score: {score:.2f}%\")\n",
        "\n",
        "        # Save to file\n",
        "        with open(full_output_path, 'w') as f:\n",
        "            json.dump(prop_scores, f, indent=4)\n",
        "        print(f\"‚úÖ Detailed scores saved to file: {full_output_path}\")\n",
        "\n",
        "        # Capture results from the display function\n",
        "        summary_stats, avg_score_by_type, single_df = display_scores_as_table(full_output_path)\n",
        "\n",
        "        # Returns the overall score and detailed statistics\n",
        "        return score, summary_stats, avg_score_by_type, single_df\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå ERROR while executing graph_similarity: {e}\")\n",
        "        return None, None, None, None # Returns None in case of error\n",
        "\n",
        "\n",
        "# --- SCRIPT EXECUTION & FINAL AGGREGATION ---\n",
        "\n",
        "# List for collecting the results of each analysis\n",
        "all_results = []\n",
        "all_dataframes = []\n",
        "\n",
        "if not os.path.exists(BASE_DRIVE_DIR):\n",
        "    print(f\"‚ùå ERRORE: La directory specificata non esiste: {BASE_DRIVE_DIR}\")\n",
        "else:\n",
        "    # Create the output folder if it does not exist\n",
        "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
        "\n",
        "    # Itera on all bunItera on all bundle foldersdle folders\n",
        "    for dirname in sorted(os.listdir(BASE_DRIVE_DIR)):\n",
        "        dirpath = os.path.join(BASE_DRIVE_DIR, dirname)\n",
        "        if os.path.isdir(dirpath):\n",
        "            print(f\"\\n{'='*25} Analyzing: {dirname} {'='*25}\")\n",
        "            ground_truth_path, predicted_path = None, None\n",
        "            try:\n",
        "                for filename in os.listdir(dirpath):\n",
        "                    if filename.endswith(\"_gt.json\"): ground_truth_path = os.path.join(dirpath, filename)\n",
        "                    elif filename.endswith(\"_pred.json\"): predicted_path = os.path.join(dirpath, filename)\n",
        "            except FileNotFoundError:\n",
        "                print(f\"Unable to access the folder '{dirname}'.\"); continue\n",
        "\n",
        "            if ground_truth_path and predicted_path:\n",
        "                output_filename = f\"prop_scores_{dirname}.json\"\n",
        "                full_output_path = os.path.join(OUTPUT_DIR, output_filename)\n",
        "\n",
        "                # Run the analysis and capture the results\n",
        "                score, summary_stats, avg_score_by_type, single_df = calculate_single_graph_score(predicted_path, ground_truth_path, full_output_path)\n",
        "\n",
        "                # Add the results to the list (only if the analysis was successful)\n",
        "                if score is not None and single_df is not None:\n",
        "                    all_results.append({\n",
        "                        \"bundle\": dirname,\n",
        "                        \"graph_similarity_score\": score,\n",
        "                    })\n",
        "                    all_dataframes.append(single_df)\n",
        "            else:\n",
        "                print(f\"Bundle pair not found.\")\n",
        "\n",
        "# --- FINAL AGGREGATE ANALYSIS ---\n",
        "if all_results and all_dataframes:\n",
        "    print(f\"\\n\\n{'='*30} FINAL AGGREGATE ANALYSIS {'='*30}\")\n",
        "\n",
        "    # Calculate the overall average similarity score\n",
        "    total_graph_score = sum(res['graph_similarity_score'] for res in all_results)\n",
        "    average_graph_score = total_graph_score / len(all_results)\n",
        "    print(f\"\\n--- Average Graph Similarity Score (on {len(all_results)} bundle) ---\")\n",
        "    print(f\"üìä Overall Average Score: {average_graph_score:.2f}%\")\n",
        "\n",
        "    # Create a unique DataFrame with ALL objects from ALL bundles\n",
        "    master_df = pd.concat(all_dataframes, ignore_index=True)\n",
        "\n",
        "    # Calculate and display overall summary statistics\n",
        "    print(f\"\\n--- Summary Statistics (on {len(master_df)} total objects) ---\")\n",
        "    total_summary_stats = master_df['Match Score (%)'].describe()\n",
        "    display(total_summary_stats.to_frame().style.format('{:,.2f}'))\n",
        "\n",
        "    # Calculate and display average performance by object type across ALL objects\n",
        "    print(\"\\n--- Average Performance by Object Type (across all objects) ---\")\n",
        "    final_avg_by_type = master_df.groupby('Type Object')['Match Score (%)'].mean().sort_values(ascending=False)\n",
        "\n",
        "    final_df_avg_type = final_avg_by_type.reset_index()\n",
        "    final_df_avg_type.columns = ['Type Object', 'Average Score (%)']\n",
        "\n",
        "    # Display the final table\n",
        "    display(final_df_avg_type.style.format({'Average Score (%)': '{:,.2f}'}).hide(axis=\"index\"))\n",
        "else:\n",
        "    print(\"\\nNo valid results to aggregate.\")"
      ],
      "metadata": {
        "id": "h_yrwh6USmv1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}